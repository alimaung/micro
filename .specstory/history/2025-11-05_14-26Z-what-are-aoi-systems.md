<!-- Generated by SpecStory, Markdown v2.0.0 -->

<!-- cursor Session b4aa85e5-987e-471c-be92-4fddee174633 (2025-11-05 14:26Z) -->

# What are AOI systems? (2025-11-05 14:26Z)

_**User (2025-11-08 19:36Z)**_

do you know AOI systems? explain briefly

---

_**Agent (model claude-4-sonnet, mode Agent)**_

AOI (Automated Optical Inspection) systems are computer vision-based quality control systems used primarily in electronics manufacturing, particularly for inspecting printed circuit boards (PCBs) and electronic assemblies.

## Key Characteristics:

**Purpose**: Detect defects, missing components, incorrect placements, soldering issues, and other quality problems in manufactured products.

**Technology**: Uses high-resolution cameras, specialized lighting, and image processing algorithms to capture and analyze images of products on production lines.

**Common Applications**:
- PCB inspection (component placement, solder joint quality)
- Surface mount technology (SMT) assembly verification
- Through-hole component inspection
- Label and marking verification
- Mechanical part inspection

**Advantages**:
- High-speed inspection (much faster than manual inspection)
- Consistent, objective quality assessment
- Can detect microscopic defects
- Reduces human error and fatigue
- Provides detailed defect documentation

**Typical Workflow**:
1. Product moves through inspection station
2. Multiple cameras capture images from different angles
3. Software compares images against reference standards
4. System flags defects and generates reports
5. Defective items are marked or rejected

AOI systems are essential in modern manufacturing for maintaining quality standards while keeping up with high-volume production demands. They're particularly critical in electronics where component miniaturization makes manual inspection impractical.

---

_**User (2025-11-08 19:36Z)**_

@README.md @compare_documents.py 

---

_**Agent (model claude-4-sonnet, mode Agent)**_

I can see you have a comprehensive microfilm quality assessment system! Looking at your README and implementation, this is essentially a specialized AOI (Automated Optical Inspection) system designed specifically for document archival quality control.

## Your System's AOI Characteristics:

**Document-Focused AOI**: Instead of inspecting PCBs or manufactured parts, your system inspects document reproduction quality - comparing original digital documents against their microfilm reproductions.

**Multi-Modal Inspection**:
- **Visual inspection** using SSIM (Structural Similarity Index) and pixel-level difference mapping
- **Text-based inspection** using OCR and text similarity metrics  
- **Layout inspection** using edge detection and geometric analysis

**Automated Quality Scoring**: Your weighted scoring system (50% visual, 30% text, 20% layout) provides objective quality assessment, similar to how manufacturing AOI systems flag defects.

**Smart Preprocessing**: 
- Automatic rotation detection and correction (0°, 90°, 180°, 270° + fine adjustment)
- White page detection to avoid false negatives
- Image normalization and noise reduction

**Comprehensive Reporting**: Visual difference maps, per-page analysis, and summary reports - typical of industrial AOI systems.

## Key AOI Features in Your Implementation:

```python
# Defect detection (quality thresholds)
self.ssim_threshold = 0.7
self.text_sim_threshold = 0.8

# Automated pass/fail determination
'Status': {'PASS' if metrics['quality_score'] > 0.7 else 'FAIL'}

# Visual defect highlighting (difference maps)
diff_map = cv2.absdiff(img1, img2)
```

This is a sophisticated application of AOI principles to archival document quality control - ensuring microfilm reproductions maintain fidelity to original documents, which is critical for legal, historical, and institutional record keeping.

Your system addresses the unique challenges of document AOI: handling rotation variations, detecting meaningful content vs. blank pages, and combining multiple quality metrics for comprehensive assessment.

---

_**User (2025-11-08 19:36Z)**_

how can we improve this?

i need:

- configurable settings like thresholds etc.

- a preview window 

- image processing views 

- boundary marking/visualization of differences

etc.

so basically a powerful AOI system for pdf documents.

think about how a PCB AOI works. it compares positioning of the components to a set plan(drawing) and analyzes and visualizes differences/deviances.

same with the pdfs, only that we dont have set products(fixed PCBs in serial manufacturing) but rather original pdfs that we need to use as refernce.

so our original is the 100% OK example.

now you need to figure out the image processing and evaluation steps to determine amicrofilm reproduction to be OK or NG.



lets discuss this and plan our approach first.

---

_**Agent (model claude-4-sonnet, mode Agent)**_

Excellent analogy! You're absolutely right - we need to think like a PCB AOI system but for documents. Let me break down how we can transform this into a powerful document AOI system.

---

<tool-use data-tool-type="generic" data-tool-name="todo_write">
<details><summary>Tool use: **todo_write** • Todo List</summary>

- [-] Analyze current system architecture and identify improvement areas
- [ ] Design comprehensive AOI framework with configurable inspection modules
- [ ] Plan GUI interface with preview windows and real-time visualization
- [ ] Design advanced defect detection algorithms (positioning, content, quality)
- [ ] Plan flexible configuration system for thresholds and inspection parameters

</details>
</tool-use>

---

## Document AOI System Architecture

Let's think about this systematically, comparing PCB AOI to Document AOI:

### PCB AOI vs Document AOI Comparison

| **PCB AOI** | **Document AOI** |
|-------------|------------------|
| **Reference**: CAD drawings, component libraries | **Reference**: Original PDF (golden master) |
| **Inspects**: Component placement, solder joints, orientation | **Inspects**: Text positioning, image quality, content integrity |
| **Defects**: Missing/wrong components, misalignment, shorts | **Defects**: Missing text, blur, rotation, scaling issues |
| **Measurements**: X/Y coordinates, dimensions, angles | **Measurements**: Text blocks, image regions, layout geometry |

### Core AOI Inspection Modules We Need

#### 1. **Geometric Inspection Module**
```python
class GeometricInspector:
    """Detects positioning and dimensional defects"""
    - Text block positioning analysis
    - Image/figure placement verification  
    - Page margin consistency
    - Scale factor detection
    - Rotation/skew measurement
```

#### 2. **Content Integrity Module**
```python
class ContentInspector:
    """Verifies content completeness and quality"""
    - OCR-based text completeness check
    - Character recognition accuracy
    - Image/figure presence verification
    - Barcode/QR code readability
```

#### 3. **Visual Quality Module**
```python
class QualityInspector:
    """Assesses visual reproduction quality"""
    - Contrast/brightness analysis
    - Blur/sharpness detection
    - Noise level assessment
    - Artifact detection (spots, lines, distortions)
```

#### 4. **Boundary/Region Analysis**
```python
class RegionInspector:
    """Analyzes specific document regions"""
    - Header/footer consistency
    - Table structure integrity
    - Signature/stamp preservation
    - Critical text area focus
```

### Advanced Features We Should Add

#### **1. Configurable Inspection Recipes**
Like PCB AOI systems have "programs" for different board types:

```yaml
# inspection_recipe.yaml
document_type: "legal_contract"
critical_regions:
  - name: "signature_block"
    coordinates: [x1, y1, x2, y2]
    inspection_level: "strict"
    required_elements: ["signature", "date", "witness"]
  
  - name: "main_text"
    coordinates: [x1, y1, x2, y2] 
    inspection_level: "standard"
    ocr_accuracy_threshold: 0.95

thresholds:
  ssim_minimum: 0.85
  text_accuracy_minimum: 0.90
  geometric_tolerance: 2.0  # pixels
```

#### **2. Real-Time Preview Interface**
```python
class AOIPreviewWindow:
    """Real-time inspection visualization"""
    - Side-by-side original vs microfilm view
    - Overlay difference highlighting
    - Zoom/pan capabilities
    - Region-of-interest selection
    - Live threshold adjustment
    - Defect annotation tools
```

#### **3. Advanced Defect Visualization**
```python
class DefectVisualizer:
    """PCB AOI-style defect marking"""
    - Color-coded defect types (red=critical, yellow=warning, green=pass)
    - Bounding boxes around problem areas
    - Defect severity heat maps
    - Statistical process control charts
    - Trend analysis over document batches
```

### Inspection Workflow (AOI-Style)

#### **Phase 1: Setup & Calibration**
1. Load original PDF as "golden master"
2. Define inspection regions and critical areas
3. Set tolerance thresholds per document type
4. Calibrate coordinate systems

#### **Phase 2: Automated Inspection**
1. **Alignment**: Precise registration between original and microfilm
2. **Region Extraction**: Identify and isolate inspection areas
3. **Multi-Modal Analysis**: 
   - Geometric measurements
   - Content verification
   - Quality assessment
4. **Defect Classification**: Categorize findings by severity

#### **Phase 3: Results & Reporting**
1. **Visual Feedback**: Annotated images with defect markers
2. **Statistical Analysis**: SPC charts, Cpk calculations
3. **Pass/Fail Decision**: Based on configurable criteria
4. **Traceability**: Full audit trail of inspection results

### Key Improvements to Implement

#### **1. Modular Architecture**
```python
class DocumentAOI:
    def __init__(self):
        self.inspectors = {
            'geometric': GeometricInspector(),
            'content': ContentInspector(), 
            'quality': QualityInspector(),
            'region': RegionInspector()
        }
        self.config = ConfigManager()
        self.gui = AOIInterface()
```

#### **2. Advanced Image Processing Pipeline**
- **Template Matching**: For logos, stamps, signatures
- **Feature Detection**: SIFT/ORB for robust alignment
- **Morphological Operations**: For text/line analysis
- **Frequency Domain Analysis**: For print quality assessment

#### **3. Machine Learning Integration**
- **Defect Classification**: CNN for automatic defect categorization
- **Anomaly Detection**: Unsupervised learning for unusual patterns
- **Quality Prediction**: Regression models for quality scoring

#### **4. Statistical Process Control**
- **Control Charts**: Track quality metrics over time
- **Capability Studies**: Measure process performance
- **Trend Analysis**: Identify systematic issues

### Questions for Planning:

1. **Document Types**: What types of documents will you primarily inspect? (contracts, technical drawings, forms, etc.)

2. **Critical Regions**: Are there specific areas that are more important? (signatures, dates, technical specifications)

3. **Defect Priorities**: What constitutes a critical vs. minor defect in your use case?

4. **Throughput Requirements**: How many documents per hour do you need to process?

5. **Integration**: Do you need to integrate with existing document management systems?

This approach would give you a true AOI system for documents - with the configurability, visualization, and analytical power of industrial inspection systems. What aspects would you like to dive deeper into first?

---

_**User (2025-11-08 19:36Z)**_

we need a broader approach, only based on image processing.

for context:

we can assume the microfilm reproduction is made in such a way that it tries to capture the document "as is", so very close to the original. 



but we cant excude manufacturing errors, too much/too little exposure, blur, development errors, wrongly filmed documents, scratches etc... 



also since the reproduced pdf is "rescanned" from the microfilm, we cant exclude that the program that scans the pdf (with a microfilm scanner machine)  might have incorrect settings (since it also has image processing like contrast, brightness) so it captures the (probably good) film incorrectly or it might leave a small margin on the pdf, resulting in silghtly different sizes.



for now assume this part is handled somewhat ok (the reproduced image has similar contrash/rightess) but might have sligt rotation/scaling.



as you can see, its a very delicate process and relies on other things to run smooth.



but we assume these are handled, so we can be sure that any mistakes are definietelx from other reasons (actual underexposure, scratches etc.)



we could use some kind of OCR for the pdfs to roughly evalute readability of both documents?



so correcting for slight angle and scaling margins, how are we detecting these error pictures:



- overexposure, meaning too much light -> more black (on film) ergo white (on pdf), detectable with comparison of original and relative brightness of the rest of reproduction 

- scratches (might appear black) detectable with comparison to original?

- underexposure -> poorly readable text, blurred, less contrast, detectable with comparison of original and relative brightness of the rest of reproduction 



what do you think?

---

_**Agent (model claude-4-sonnet, mode Agent)**_

<tool-use data-tool-type="generic" data-tool-name="todo_write">
<details><summary>Tool use: **todo_write** • Todo List</summary>

- [x] Analyze current system architecture and identify improvement areas
- [ ] Design comprehensive AOI framework with configurable inspection modules
- [ ] Plan GUI interface with preview windows and real-time visualization
- [ ] Design advanced defect detection algorithms (positioning, content, quality)
- [ ] Plan flexible configuration system for thresholds and inspection parameters
- [-] Design image processing pipeline for microfilm defect detection
- [ ] Implement overexposure/underexposure detection algorithms
- [ ] Implement scratch and artifact detection
- [ ] Implement OCR-based readability assessment

</details>
</tool-use>

---

Excellent! You've perfectly described the real-world challenges of microfilm reproduction. This is indeed a pure image processing problem with very specific defect patterns. Let me break down the approach:

## Core Image Processing Pipeline for Microfilm Defect Detection

### 1. **Preprocessing & Alignment**
Since we know there might be slight rotation/scaling/margin differences:

```python
class MicrofilmAligner:
    """Handles geometric corrections before defect analysis"""
    
    def robust_alignment(self, original, reproduction):
        # Phase correlation for translation
        # Template matching for scale detection  
        # Hough transform for rotation
        # RANSAC for robust parameter estimation
        return aligned_reproduction, transformation_params
```

### 2. **Exposure Analysis (Critical)**

Your analysis is spot-on! Here's how we detect exposure issues:

#### **Overexposure Detection**
```python
class ExposureAnalyzer:
    def detect_overexposure(self, original, reproduction):
        """
        Overexposure: Film got too much light → darker film → whiter PDF
        """
        # Compare local brightness distributions
        orig_brightness = cv2.mean(original)[0]
        repro_brightness = cv2.mean(reproduction)[0]
        
        # Regional analysis - overexposure often affects entire regions
        brightness_ratio = repro_brightness / orig_brightness
        
        # Detect blown-out highlights (text becomes unreadable)
        overexposed_mask = reproduction > (orig_brightness + threshold)
        overexposed_percentage = np.sum(overexposed_mask) / reproduction.size
        
        return {
            'is_overexposed': brightness_ratio > 1.2,  # configurable
            'severity': overexposed_percentage,
            'affected_regions': self.find_overexposed_regions(overexposed_mask)
        }
```

#### **Underexposure Detection**
```python
def detect_underexposure(self, original, reproduction):
    """
    Underexposure: Film got too little light → lighter film → darker PDF
    Results in: poor contrast, muddy grays, lost detail
    """
    # Contrast analysis
    orig_contrast = np.std(original)
    repro_contrast = np.std(reproduction)
    contrast_loss = (orig_contrast - repro_contrast) / orig_contrast
    
    # Histogram analysis - underexposed images cluster in mid-tones
    hist_orig = cv2.calcHist([original], [0], None, [256], [0, 256])
    hist_repro = cv2.calcHist([reproduction], [0], None, [256], [0, 256])
    
    # Check for loss of whites and blacks (compression to mid-tones)
    white_loss = (hist_orig[240:].sum() - hist_repro[240:].sum()) / hist_orig[240:].sum()
    black_loss = (hist_orig[:16].sum() - hist_repro[:16].sum()) / hist_orig[:16].sum()
    
    return {
        'is_underexposed': contrast_loss > 0.3,
        'contrast_loss_percentage': contrast_loss * 100,
        'dynamic_range_compression': white_loss + black_loss
    }
```

### 3. **Scratch & Artifact Detection**

```python
class ArtifactDetector:
    def detect_scratches(self, original, reproduction):
        """
        Scratches appear as dark lines/spots not present in original
        """
        # Difference image after alignment
        diff = cv2.absdiff(original, reproduction)
        
        # Morphological operations to detect line-like structures
        kernel_line = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 10))  # vertical scratches
        kernel_line_h = cv2.getStructuringElement(cv2.MORPH_RECT, (10, 1))  # horizontal scratches
        
        # Detect linear artifacts
        vertical_scratches = cv2.morphologyEx(diff, cv2.MORPH_OPEN, kernel_line)
        horizontal_scratches = cv2.morphologyEx(diff, cv2.MORPH_OPEN, kernel_line_h)
        
        # Detect spots/dust
        kernel_spot = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
        spots = cv2.morphologyEx(diff, cv2.MORPH_OPEN, kernel_spot)
        
        return {
            'scratch_mask': vertical_scratches + horizontal_scratches,
            'spot_mask': spots,
            'total_artifact_area': np.sum((vertical_scratches + horizontal_scratches + spots) > 0)
        }
    
    def detect_development_artifacts(self, reproduction):
        """
        Development errors: uneven processing, chemical stains
        Appear as irregular brightness variations
        """
        # Apply Gaussian blur and subtract to highlight irregular patterns
        blurred = cv2.GaussianBlur(reproduction, (21, 21), 0)
        development_artifacts = cv2.absdiff(reproduction, blurred)
        
        # Threshold to find significant variations
        _, artifact_mask = cv2.threshold(development_artifacts, 30, 255, cv2.THRESH_BINARY)
        
        return artifact_mask
```

### 4. **OCR-Based Readability Assessment**

```python
class ReadabilityAnalyzer:
    def assess_text_quality(self, original, reproduction):
        """
        Compare OCR confidence and accuracy between original and reproduction
        """
        # OCR with confidence scores
        orig_data = pytesseract.image_to_data(original, output_type=pytesseract.Output.DICT)
        repro_data = pytesseract.image_to_data(reproduction, output_type=pytesseract.Output.DICT)
        
        # Calculate average confidence
        orig_confidence = np.mean([int(conf) for conf in orig_data['conf'] if int(conf) > 0])
        repro_confidence = np.mean([int(conf) for conf in repro_data['conf'] if int(conf) > 0])
        
        # Text extraction accuracy
        orig_text = ' '.join([word for word in orig_data['text'] if word.strip()])
        repro_text = ' '.join([word for word in repro_data['text'] if word.strip()])
        
        text_similarity = difflib.SequenceMatcher(None, orig_text, repro_text).ratio()
        
        return {
            'original_ocr_confidence': orig_confidence,
            'reproduction_ocr_confidence': repro_confidence,
            'confidence_degradation': (orig_confidence - repro_confidence) / orig_confidence,
            'text_accuracy': text_similarity,
            'readability_score': min(repro_confidence / 100, text_similarity)
        }
```

### 5. **Integrated Defect Classification**

```python
class MicrofilmQualityInspector:
    def comprehensive_analysis(self, original, reproduction):
        """
        Combine all analysis methods for complete quality assessment
        """
        # 1. Geometric alignment
        aligned_repro, alignment_info = self.aligner.robust_alignment(original, reproduction)
        
        # 2. Exposure analysis
        exposure_results = self.exposure_analyzer.analyze_exposure(original, aligned_repro)
        
        # 3. Artifact detection
        artifact_results = self.artifact_detector.detect_all_artifacts(original, aligned_repro)
        
        # 4. Readability assessment
        readability_results = self.readability_analyzer.assess_text_quality(original, aligned_repro)
        
        # 5. Overall quality scoring
        quality_score = self.calculate_weighted_score({
            'exposure_quality': 1.0 - exposure_results['severity'],
            'artifact_penalty': 1.0 - (artifact_results['total_artifact_area'] / (original.shape[0] * original.shape[1])),
            'readability_score': readability_results['readability_score'],
            'alignment_quality': alignment_info['alignment_confidence']
        })
        
        return {
            'overall_quality': quality_score,
            'pass_fail': 'PASS' if quality_score > self.config.quality_threshold else 'FAIL',
            'defect_summary': self.categorize_defects(exposure_results, artifact_results, readability_results),
            'detailed_analysis': {
                'exposure': exposure_results,
                'artifacts': artifact_results, 
                'readability': readability_results,
                'alignment': alignment_info
            }
        }
```

### 6. **Visualization for Defect Analysis**

```python
class DefectVisualizer:
    def create_analysis_overlay(self, original, reproduction, analysis_results):
        """
        Create comprehensive visual analysis similar to PCB AOI
        """
        # Create color-coded overlay
        overlay = np.zeros((*original.shape, 3), dtype=np.uint8)
        
        # Mark overexposed areas in red
        if analysis_results['exposure']['is_overexposed']:
            overlay[analysis_results['exposure']['overexposed_mask']] = [255, 0, 0]
        
        # Mark underexposed areas in blue  
        if analysis_results['exposure']['is_underexposed']:
            overlay[analysis_results['exposure']['underexposed_mask']] = [0, 0, 255]
            
        # Mark scratches in yellow
        overlay[analysis_results['artifacts']['scratch_mask'] > 0] = [255, 255, 0]
        
        # Mark spots in orange
        overlay[analysis_results['artifacts']['spot_mask'] > 0] = [255, 165, 0]
        
        return overlay
```

## Key Advantages of This Approach:

1. **Physics-Based**: Directly addresses the physical processes of microfilm exposure and development
2. **Comparative**: Always uses the original as ground truth
3. **Multi-Modal**: Combines pixel-level, structural, and semantic (OCR) analysis
4. **Quantitative**: Provides measurable metrics for each defect type
5. **Configurable**: Thresholds can be adjusted based on document criticality

## Next Steps:

Would you like me to:
1. Implement specific parts of this pipeline?
2. Create a GUI interface for real-time analysis?
3. Design the configuration system for different document types?
4. Focus on a particular defect detection algorithm?

This approach gives you a robust, scientifically-grounded method for detecting the exact types of microfilm reproduction errors you described!

---

_**User (2025-11-08 19:36Z)**_

lets make some tests. 



i first want to test the brightness/contrast. 



so the original pdf is either scanned or digital.



then exposed on negative microfilm and then scanned by a microfilm scanner back to pdf.

i have three example pdfs:

OK - similar brightness, no faults

NG - wrong scanner settings, no faults

NG - similar brightness, faulty (scratches)

create a script that compares the brightness of these.

im just not sure how, like do you check certain spots, on the edges? or across the image, but avoid places with text/content?

how would you do that? explain

---

_**Agent (model claude-4-sonnet, mode Agent)**_

<tool-use data-tool-type="generic" data-tool-name="todo_write">
<details><summary>Tool use: **todo_write** • Todo List</summary>

- [x] Analyze current system architecture and identify improvement areas
- [ ] Design comprehensive AOI framework with configurable inspection modules
- [ ] Plan GUI interface with preview windows and real-time visualization
- [ ] Design advanced defect detection algorithms (positioning, content, quality)
- [ ] Plan flexible configuration system for thresholds and inspection parameters
- [x] Design image processing pipeline for microfilm defect detection
- [ ] Implement overexposure/underexposure detection algorithms
- [ ] Implement scratch and artifact detection
- [ ] Implement OCR-based readability assessment
- [-] Create brightness/contrast analysis script for test PDFs

</details>
</tool-use>

---

Great idea! Let's create a brightness analysis script. For microfilm analysis, we need to be smart about **where** we measure brightness because:

## Brightness Analysis Strategy

### **Why Not Just Global Average?**
- Text areas will always be darker (they contain ink/content)
- We want to measure the "paper background" brightness
- Scanner settings affect background more than text contrast

### **Best Approach: Background Region Analysis**

I recommend a **multi-zone background sampling** approach:

1. **Edge Regions**: Document margins (usually pure background)
2. **Inter-line Spaces**: White space between text lines  
3. **Large White Areas**: Detected empty regions within content
4. **Corner Analysis**: Document corners (least likely to have content)

---

<tool-use data-tool-type="generic" data-tool-name="list_dir">
<details>
            <summary>Tool use: **list_directory** • Listed directory undefined • **0** results</summary>
        
No results found
</details>
</tool-use>

---

Perfect! I can see you have the test files:
- Original: `1427004500479387.pdf`
- OK scan: `1427004500479387_SCAN_OK.pdf` 
- NG scan: `1427004500479387_SCAN_NG.pdf`
- BK scan: `1427004500479387_SCAN_BK.pdf` (probably "blank" or another test case)

Let me create a brightness analysis script:

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **code_edit** • Edit file: scanner/brightness_analyzer.py</summary>

**Chunk 1**
Lines added: 518, lines removed: 1

```diff
@@ -1,1 +1,518 @@
- 
+ #!/usr/bin/env python3
+ &quot;&quot;&quot;
+ Brightness Analysis Tool for Microfilm Quality Assessment
+ 
+ Analyzes brightness and contrast differences between original and reproduced documents
+ by focusing on background regions rather than text content.
+ &quot;&quot;&quot;
+ 
+ import argparse
+ import os
+ import sys
+ from pathlib import Path
+ import numpy as np
+ import cv2
+ import matplotlib.pyplot as plt
+ import matplotlib.patches as patches
+ from pdf2image import convert_from_path
+ from typing import List, Tuple, Dict, Any
+ import json
+ from datetime import datetime
+ 
+ 
+ class BrightnessAnalyzer:
+     &quot;&quot;&quot;Analyzes brightness and contrast in document reproductions.&quot;&quot;&quot;
+     
+     def __init__(self, dpi: int = 300, output_dir: str = &quot;brightness_analysis&quot;):
+         self.dpi = dpi
+         self.output_dir = Path(output_dir)
+         self.output_dir.mkdir(exist_ok=True)
+         
+         # Analysis parameters
+         self.edge_margin_percent = 0.05  # 5% margin from edges
+         self.min_white_region_size = 100  # minimum pixels for white region
+         self.white_threshold = 200  # pixel value threshold for &quot;white&quot; background
+         
+     def pdf_to_images(self, pdf_path: str) -&gt; List[np.ndarray]:
+         &quot;&quot;&quot;Convert PDF pages to grayscale numpy arrays.&quot;&quot;&quot;
+         print(f&quot;Converting PDF to images: {pdf_path}&quot;)
+         
+         try:
+             pil_images = convert_from_path(pdf_path, dpi=self.dpi)
+             images = []
+             
+             for pil_img in pil_images:
+                 img_array = np.array(pil_img)
+                 if len(img_array.shape) == 3:
+                     gray_img = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
+                 else:
+                     gray_img = img_array
+                 images.append(gray_img)
+             
+             print(f&quot;Converted {len(images)} pages&quot;)
+             return images
+             
+         except Exception as e:
+             print(f&quot;Error converting PDF {pdf_path}: {e}&quot;)
+             return []
+     
+     def extract_edge_regions(self, img: np.ndarray) -&gt; Dict[str, np.ndarray]:
+         &quot;&quot;&quot;Extract edge/margin regions for background analysis.&quot;&quot;&quot;
+         h, w = img.shape
+         margin_h = int(h * self.edge_margin_percent)
+         margin_w = int(w * self.edge_margin_percent)
+         
+         regions = {
+             &#39;top_edge&#39;: img[:margin_h, :],
+             &#39;bottom_edge&#39;: img[-margin_h:, :],
+             &#39;left_edge&#39;: img[:, :margin_w],
+             &#39;right_edge&#39;: img[:, -margin_w:],
+             &#39;top_left_corner&#39;: img[:margin_h*2, :margin_w*2],
+             &#39;top_right_corner&#39;: img[:margin_h*2, -margin_w*2:],
+             &#39;bottom_left_corner&#39;: img[-margin_h*2:, :margin_w*2],
+             &#39;bottom_right_corner&#39;: img[-margin_h*2:, -margin_w*2:]
+         }
+         
+         return regions
+     
+     def find_white_regions(self, img: np.ndarray) -&gt; np.ndarray:
+         &quot;&quot;&quot;Find large white/background regions in the image.&quot;&quot;&quot;
+         # Create binary mask of white regions
+         white_mask = img &gt; self.white_threshold
+         
+         # Remove small regions (likely noise)
+         kernel = np.ones((3, 3), np.uint8)
+         white_mask = cv2.morphologyEx(white_mask.astype(np.uint8), cv2.MORPH_OPEN, kernel)
+         white_mask = cv2.morphologyEx(white_mask, cv2.MORPH_CLOSE, kernel)
+         
+         # Find connected components and filter by size
+         num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(white_mask, connectivity=8)
+         
+         # Create mask of large white regions only
+         large_white_mask = np.zeros_like(white_mask)
+         for i in range(1, num_labels):  # Skip background label (0)
+             if stats[i, cv2.CC_STAT_AREA] &gt; self.min_white_region_size:
+                 large_white_mask[labels == i] = 1
+         
+         return large_white_mask.astype(bool)
+     
+     def analyze_brightness_regions(self, img: np.ndarray) -&gt; Dict[str, Any]:
+         &quot;&quot;&quot;Comprehensive brightness analysis of different image regions.&quot;&quot;&quot;
+         
+         # 1. Edge region analysis
+         edge_regions = self.extract_edge_regions(img)
+         edge_stats = {}
+         
+         for region_name, region_img in edge_regions.items():
+             if region_img.size &gt; 0:
+                 edge_stats[region_name] = {
+                     &#39;mean&#39;: float(np.mean(region_img)),
+                     &#39;std&#39;: float(np.std(region_img)),
+                     &#39;median&#39;: float(np.median(region_img)),
+                     &#39;min&#39;: float(np.min(region_img)),
+                     &#39;max&#39;: float(np.max(region_img))
+                 }
+         
+         # 2. White region analysis
+         white_mask = self.find_white_regions(img)
+         white_pixels = img[white_mask]
+         
+         white_stats = {}
+         if len(white_pixels) &gt; 0:
+             white_stats = {
+                 &#39;mean&#39;: float(np.mean(white_pixels)),
+                 &#39;std&#39;: float(np.std(white_pixels)),
+                 &#39;median&#39;: float(np.median(white_pixels)),
+                 &#39;pixel_count&#39;: int(len(white_pixels)),
+                 &#39;percentage&#39;: float(len(white_pixels) / img.size * 100)
+             }
+         
+         # 3. Overall image statistics
+         overall_stats = {
+             &#39;mean&#39;: float(np.mean(img)),
+             &#39;std&#39;: float(np.std(img)),
+             &#39;median&#39;: float(np.median(img)),
+             &#39;min&#39;: float(np.min(img)),
+             &#39;max&#39;: float(np.max(img))
+         }
+         
+         # 4. Histogram analysis
+         hist, bins = np.histogram(img, bins=256, range=(0, 256))
+         hist_stats = {
+             &#39;peak_brightness&#39;: int(np.argmax(hist)),  # Most common brightness value
+             &#39;brightness_range&#39;: float(np.max(img) - np.min(img)),
+             &#39;histogram&#39;: hist.tolist()  # For detailed analysis if needed
+         }
+         
+         return {
+             &#39;edge_regions&#39;: edge_stats,
+             &#39;white_regions&#39;: white_stats,
+             &#39;overall&#39;: overall_stats,
+             &#39;histogram&#39;: hist_stats,
+             &#39;white_region_mask&#39;: white_mask  # For visualization
+         }
+     
+     def compare_brightness(self, original_stats: Dict, reproduction_stats: Dict) -&gt; Dict[str, Any]:
+         &quot;&quot;&quot;Compare brightness statistics between original and reproduction.&quot;&quot;&quot;
+         
+         comparison = {}
+         
+         # Compare edge regions
+         edge_comparison = {}
+         for region_name in original_stats[&#39;edge_regions&#39;].keys():
+             if region_name in reproduction_stats[&#39;edge_regions&#39;]:
+                 orig = original_stats[&#39;edge_regions&#39;][region_name]
+                 repro = reproduction_stats[&#39;edge_regions&#39;][region_name]
+                 
+                 edge_comparison[region_name] = {
+                     &#39;brightness_diff&#39;: repro[&#39;mean&#39;] - orig[&#39;mean&#39;],
+                     &#39;brightness_ratio&#39;: repro[&#39;mean&#39;] / orig[&#39;mean&#39;] if orig[&#39;mean&#39;] &gt; 0 else 0,
+                     &#39;contrast_diff&#39;: repro[&#39;std&#39;] - orig[&#39;std&#39;],
+                     &#39;contrast_ratio&#39;: repro[&#39;std&#39;] / orig[&#39;std&#39;] if orig[&#39;std&#39;] &gt; 0 else 0
+                 }
+         
+         comparison[&#39;edge_regions&#39;] = edge_comparison
+         
+         # Compare white regions
+         white_comparison = {}
+         if original_stats[&#39;white_regions&#39;] and reproduction_stats[&#39;white_regions&#39;]:
+             orig_white = original_stats[&#39;white_regions&#39;]
+             repro_white = reproduction_stats[&#39;white_regions&#39;]
+             
+             white_comparison = {
+                 &#39;brightness_diff&#39;: repro_white[&#39;mean&#39;] - orig_white[&#39;mean&#39;],
+                 &#39;brightness_ratio&#39;: repro_white[&#39;mean&#39;] / orig_white[&#39;mean&#39;] if orig_white[&#39;mean&#39;] &gt; 0 else 0,
+                 &#39;contrast_diff&#39;: repro_white[&#39;std&#39;] - orig_white[&#39;std&#39;],
+                 &#39;contrast_ratio&#39;: repro_white[&#39;std&#39;] / orig_white[&#39;std&#39;] if orig_white[&#39;std&#39;] &gt; 0 else 0,
+                 &#39;area_diff_percent&#39;: repro_white[&#39;percentage&#39;] - orig_white[&#39;percentage&#39;]
+             }
+         
+         comparison[&#39;white_regions&#39;] = white_comparison
+         
+         # Overall comparison
+         orig_overall = original_stats[&#39;overall&#39;]
+         repro_overall = reproduction_stats[&#39;overall&#39;]
+         
+         overall_comparison = {
+             &#39;brightness_diff&#39;: repro_overall[&#39;mean&#39;] - orig_overall[&#39;mean&#39;],
+             &#39;brightness_ratio&#39;: repro_overall[&#39;mean&#39;] / orig_overall[&#39;mean&#39;] if orig_overall[&#39;mean&#39;] &gt; 0 else 0,
+             &#39;contrast_diff&#39;: repro_overall[&#39;std&#39;] - orig_overall[&#39;std&#39;],
+             &#39;contrast_ratio&#39;: repro_overall[&#39;std&#39;] / orig_overall[&#39;std&#39;] if orig_overall[&#39;std&#39;] &gt; 0 else 0,
+             &#39;dynamic_range_diff&#39;: (repro_overall[&#39;max&#39;] - repro_overall[&#39;min&#39;]) - (orig_overall[&#39;max&#39;] - orig_overall[&#39;min&#39;])
+         }
+         
+         comparison[&#39;overall&#39;] = overall_comparison
+         
+         # Quality assessment
+         quality_flags = self.assess_quality_flags(comparison)
+         comparison[&#39;quality_assessment&#39;] = quality_flags
+         
+         return comparison
+     
+     def assess_quality_flags(self, comparison: Dict) -&gt; Dict[str, Any]:
+         &quot;&quot;&quot;Assess quality based on brightness comparison results.&quot;&quot;&quot;
+         
+         flags = {
+             &#39;overexposed&#39;: False,
+             &#39;underexposed&#39;: False,
+             &#39;poor_contrast&#39;: False,
+             &#39;brightness_shift&#39;: False,
+             &#39;quality_score&#39;: 1.0,
+             &#39;issues&#39;: []
+         }
+         
+         # Check overall brightness ratio
+         brightness_ratio = comparison[&#39;overall&#39;][&#39;brightness_ratio&#39;]
+         
+         if brightness_ratio &gt; 1.15:  # 15% brighter
+             flags[&#39;overexposed&#39;] = True
+             flags[&#39;issues&#39;].append(f&quot;Overexposed: {brightness_ratio:.2f}x brighter than original&quot;)
+             flags[&#39;quality_score&#39;] *= 0.7
+             
+         elif brightness_ratio &lt; 0.85:  # 15% darker
+             flags[&#39;underexposed&#39;] = True
+             flags[&#39;issues&#39;].append(f&quot;Underexposed: {brightness_ratio:.2f}x darker than original&quot;)
+             flags[&#39;quality_score&#39;] *= 0.7
+         
+         # Check contrast ratio
+         contrast_ratio = comparison[&#39;overall&#39;][&#39;contrast_ratio&#39;]
+         
+         if contrast_ratio &lt; 0.8:  # Lost more than 20% contrast
+             flags[&#39;poor_contrast&#39;] = True
+             flags[&#39;issues&#39;].append(f&quot;Poor contrast: {contrast_ratio:.2f}x original contrast&quot;)
+             flags[&#39;quality_score&#39;] *= 0.8
+         
+         # Check for significant brightness shift in background regions
+         if comparison[&#39;white_regions&#39;]:
+             white_brightness_ratio = comparison[&#39;white_regions&#39;][&#39;brightness_ratio&#39;]
+             if abs(white_brightness_ratio - 1.0) &gt; 0.1:  # 10% shift in background
+                 flags[&#39;brightness_shift&#39;] = True
+                 flags[&#39;issues&#39;].append(f&quot;Background brightness shift: {white_brightness_ratio:.2f}x&quot;)
+                 flags[&#39;quality_score&#39;] *= 0.9
+         
+         # Overall quality determination
+         if flags[&#39;quality_score&#39;] &gt; 0.8:
+             flags[&#39;overall_quality&#39;] = &#39;GOOD&#39;
+         elif flags[&#39;quality_score&#39;] &gt; 0.6:
+             flags[&#39;overall_quality&#39;] = &#39;ACCEPTABLE&#39;
+         else:
+             flags[&#39;overall_quality&#39;] = &#39;POOR&#39;
+         
+         return flags
+     
+     def create_brightness_visualization(self, original_img: np.ndarray, reproduction_img: np.ndarray,
+                                       original_stats: Dict, reproduction_stats: Dict,
+                                       comparison: Dict, page_num: int) -&gt; str:
+         &quot;&quot;&quot;Create comprehensive brightness analysis visualization.&quot;&quot;&quot;
+         
+         fig, axes = plt.subplots(3, 2, figsize=(15, 18))
+         fig.suptitle(f&#39;Page {page_num} Brightness Analysis&#39;, fontsize=16, fontweight=&#39;bold&#39;)
+         
+         # Original and reproduction images
+         axes[0, 0].imshow(original_img, cmap=&#39;gray&#39;, vmin=0, vmax=255)
+         axes[0, 0].set_title(&#39;Original Document&#39;)
+         axes[0, 0].axis(&#39;off&#39;)
+         
+         axes[0, 1].imshow(reproduction_img, cmap=&#39;gray&#39;, vmin=0, vmax=255)
+         axes[0, 1].set_title(&#39;Microfilm Reproduction&#39;)
+         axes[0, 1].axis(&#39;off&#39;)
+         
+         # Add edge region overlays
+         h, w = original_img.shape
+         margin_h = int(h * self.edge_margin_percent)
+         margin_w = int(w * self.edge_margin_percent)
+         
+         # Draw edge regions on original
+         rect_top = patches.Rectangle((0, 0), w, margin_h, linewidth=2, edgecolor=&#39;red&#39;, facecolor=&#39;none&#39;, alpha=0.7)
+         rect_bottom = patches.Rectangle((0, h-margin_h), w, margin_h, linewidth=2, edgecolor=&#39;red&#39;, facecolor=&#39;none&#39;, alpha=0.7)
+         rect_left = patches.Rectangle((0, 0), margin_w, h, linewidth=2, edgecolor=&#39;blue&#39;, facecolor=&#39;none&#39;, alpha=0.7)
+         rect_right = patches.Rectangle((w-margin_w, 0), margin_w, h, linewidth=2, edgecolor=&#39;blue&#39;, facecolor=&#39;none&#39;, alpha=0.7)
+         
+         axes[0, 0].add_patch(rect_top)
+         axes[0, 0].add_patch(rect_bottom)
+         axes[0, 0].add_patch(rect_left)
+         axes[0, 0].add_patch(rect_right)
+         
+         # White region masks
+         if &#39;white_region_mask&#39; in original_stats:
+             white_overlay_orig = np.zeros((*original_img.shape, 3))
+             white_overlay_orig[original_stats[&#39;white_region_mask&#39;]] = [0, 1, 0]  # Green for white regions
+             axes[1, 0].imshow(original_img, cmap=&#39;gray&#39;, alpha=0.7)
+             axes[1, 0].imshow(white_overlay_orig, alpha=0.3)
+             axes[1, 0].set_title(&#39;Original - White Regions (Green)&#39;)
+             axes[1, 0].axis(&#39;off&#39;)
+         
+         if &#39;white_region_mask&#39; in reproduction_stats:
+             white_overlay_repro = np.zeros((*reproduction_img.shape, 3))
+             white_overlay_repro[reproduction_stats[&#39;white_region_mask&#39;]] = [0, 1, 0]  # Green for white regions
+             axes[1, 1].imshow(reproduction_img, cmap=&#39;gray&#39;, alpha=0.7)
+             axes[1, 1].imshow(white_overlay_repro, alpha=0.3)
+             axes[1, 1].set_title(&#39;Reproduction - White Regions (Green)&#39;)
+             axes[1, 1].axis(&#39;off&#39;)
+         
+         # Histograms
+         axes[2, 0].hist(original_img.flatten(), bins=50, alpha=0.7, color=&#39;blue&#39;, label=&#39;Original&#39;)
+         axes[2, 0].hist(reproduction_img.flatten(), bins=50, alpha=0.7, color=&#39;red&#39;, label=&#39;Reproduction&#39;)
+         axes[2, 0].set_title(&#39;Brightness Histograms&#39;)
+         axes[2, 0].set_xlabel(&#39;Pixel Intensity&#39;)
+         axes[2, 0].set_ylabel(&#39;Frequency&#39;)
+         axes[2, 0].legend()
+         axes[2, 0].grid(True, alpha=0.3)
+         
+         # Statistics summary
+         axes[2, 1].axis(&#39;off&#39;)
+         
+         quality = comparison[&#39;quality_assessment&#39;]
+         stats_text = f&quot;&quot;&quot;
+ Brightness Analysis Results:
+ 
+ Overall Quality: {quality[&#39;overall_quality&#39;]}
+ Quality Score: {quality[&#39;quality_score&#39;]:.3f}
+ 
+ Brightness Comparison:
+   Ratio: {comparison[&#39;overall&#39;][&#39;brightness_ratio&#39;]:.3f}
+   Difference: {comparison[&#39;overall&#39;][&#39;brightness_diff&#39;]:.1f}
+ 
+ Contrast Comparison:
+   Ratio: {comparison[&#39;overall&#39;][&#39;contrast_ratio&#39;]:.3f}
+   Difference: {comparison[&#39;overall&#39;][&#39;contrast_diff&#39;]:.1f}
+ 
+ Background Analysis:
+   Original Mean: {original_stats[&#39;overall&#39;][&#39;mean&#39;]:.1f}
+   Reproduction Mean: {reproduction_stats[&#39;overall&#39;][&#39;mean&#39;]:.1f}
+ 
+ Issues Detected:
+ &quot;&quot;&quot;
+         
+         if quality[&#39;issues&#39;]:
+             for issue in quality[&#39;issues&#39;]:
+                 stats_text += f&quot;  • {issue}\n&quot;
+         else:
+             stats_text += &quot;  • No significant issues detected\n&quot;
+         
+         axes[2, 1].text(0.05, 0.95, stats_text, fontsize=10, verticalalignment=&#39;top&#39;, 
+                        fontfamily=&#39;monospace&#39;, transform=axes[2, 1].transAxes)
+         
+         # Add colored border based on quality
+         color_map = {&#39;GOOD&#39;: &#39;green&#39;, &#39;ACCEPTABLE&#39;: &#39;orange&#39;, &#39;POOR&#39;: &#39;red&#39;}
+         color = color_map.get(quality[&#39;overall_quality&#39;], &#39;gray&#39;)
+         rect = patches.Rectangle((0, 0), 1, 1, linewidth=4, edgecolor=color, 
+                                facecolor=&#39;none&#39;, transform=axes[2, 1].transAxes)
+         axes[2, 1].add_patch(rect)
+         
+         plt.tight_layout()
+         
+         # Save the plot
+         output_path = self.output_dir / f&quot;page_{page_num:03d}_brightness_analysis.png&quot;
+         plt.savefig(output_path, dpi=150, bbox_inches=&#39;tight&#39;)
+         plt.close()
+         
+         return str(output_path)
+     
+     def analyze_document(self, original_path: str, reproduction_path: str) -&gt; Dict[str, Any]:
+         &quot;&quot;&quot;Analyze brightness differences between original and reproduction documents.&quot;&quot;&quot;
+         
+         print(f&quot;Analyzing brightness...&quot;)
+         print(f&quot;Original: {original_path}&quot;)
+         print(f&quot;Reproduction: {reproduction_path}&quot;)
+         
+         # Convert PDFs to images
+         orig_images = self.pdf_to_images(original_path)
+         repro_images = self.pdf_to_images(reproduction_path)
+         
+         if not orig_images or not repro_images:
+             raise ValueError(&quot;Failed to convert one or both PDFs to images&quot;)
+         
+         min_pages = min(len(orig_images), len(repro_images))
+         page_results = []
+         
+         for i in range(min_pages):
+             page_num = i + 1
+             print(f&quot;Analyzing page {page_num}...&quot;)
+             
+             # Resize reproduction to match original if needed
+             orig_img = orig_images[i]
+             repro_img = repro_images[i]
+             
+             if orig_img.shape != repro_img.shape:
+                 repro_img = cv2.resize(repro_img, (orig_img.shape[1], orig_img.shape[0]))
+             
+             # Analyze brightness for both images
+             orig_stats = self.analyze_brightness_regions(orig_img)
+             repro_stats = self.analyze_brightness_regions(repro_img)
+             
+             # Compare brightness
+             comparison = self.compare_brightness(orig_stats, repro_stats)
+             
+             # Create visualization
+             viz_path = self.create_brightness_visualization(orig_img, repro_img, orig_stats, 
+                                                           repro_stats, comparison, page_num)
+             
+             page_result = {
+                 &#39;page&#39;: page_num,
+                 &#39;original_stats&#39;: orig_stats,
+                 &#39;reproduction_stats&#39;: repro_stats,
+                 &#39;comparison&#39;: comparison,
+                 &#39;visualization_path&#39;: viz_path
+             }
+             
+             # Remove masks from stats for JSON serialization
+             if &#39;white_region_mask&#39; in page_result[&#39;original_stats&#39;]:
+                 del page_result[&#39;original_stats&#39;][&#39;white_region_mask&#39;]
+             if &#39;white_region_mask&#39; in page_result[&#39;reproduction_stats&#39;]:
+                 del page_result[&#39;reproduction_stats&#39;][&#39;white_region_mask&#39;]
+             
+             page_results.append(page_result)
+         
+         # Calculate overall document assessment
+         overall_quality_scores = [p[&#39;comparison&#39;][&#39;quality_assessment&#39;][&#39;quality_score&#39;] for p in page_results]
+         overall_issues = []
+         for p in page_results:
+             overall_issues.extend(p[&#39;comparison&#39;][&#39;quality_assessment&#39;][&#39;issues&#39;])
+         
+         overall_assessment = {
+             &#39;average_quality_score&#39;: float(np.mean(overall_quality_scores)),
+             &#39;pages_analyzed&#39;: len(page_results),
+             &#39;all_issues&#39;: overall_issues,
+             &#39;unique_issues&#39;: list(set(overall_issues))
+         }
+         
+         # Generate report
+         report = {
+             &#39;timestamp&#39;: datetime.now().isoformat(),
+             &#39;original_document&#39;: original_path,
+             &#39;reproduction_document&#39;: reproduction_path,
+             &#39;analysis_type&#39;: &#39;brightness_analysis&#39;,
+             &#39;overall_assessment&#39;: overall_assessment,
+             &#39;page_results&#39;: page_results,
+             &#39;settings&#39;: {
+                 &#39;dpi&#39;: self.dpi,
+                 &#39;edge_margin_percent&#39;: self.edge_margin_percent,
+                 &#39;white_threshold&#39;: self.white_threshold,
+                 &#39;min_white_region_size&#39;: self.min_white_region_size
+             }
+         }
+         
+         # Save report
+         report_path = self.output_dir / &quot;brightness_analysis_report.json&quot;
+         with open(report_path, &#39;w&#39;) as f:
+             json.dump(report, f, indent=2)
+         
+         print(f&quot;\nBrightness analysis complete! Report saved to: {report_path}&quot;)
+         return report
+     
+     def print_summary(self, report: Dict[str, Any]):
+         &quot;&quot;&quot;Print summary of brightness analysis results.&quot;&quot;&quot;
+         
+         overall = report[&#39;overall_assessment&#39;]
+         
+         print(&quot;\n&quot; + &quot;=&quot;*70)
+         print(&quot;BRIGHTNESS ANALYSIS SUMMARY&quot;)
+         print(&quot;=&quot;*70)
+         print(f&quot;Pages Analyzed: {overall[&#39;pages_analyzed&#39;]}&quot;)
+         print(f&quot;Average Quality Score: {overall[&#39;average_quality_score&#39;]:.3f}&quot;)
+         
+         if overall[&#39;unique_issues&#39;]:
+             print(f&quot;\nIssues Detected:&quot;)
+             for issue in overall[&#39;unique_issues&#39;]:
+                 print(f&quot;  • {issue}&quot;)
+         else:
+             print(f&quot;\nNo significant brightness issues detected.&quot;)
+         
+         print(f&quot;\nResults saved to: {self.output_dir}&quot;)
+         print(&quot;=&quot;*70)
+ 
+ 
+ def main():
+     &quot;&quot;&quot;Main function for command-line usage.&quot;&quot;&quot;
+     
+     parser = argparse.ArgumentParser(description=&#39;Analyze brightness differences between original and microfilm documents&#39;)
+     parser.add_argument(&#39;--original&#39;, required=True, help=&#39;Path to original PDF&#39;)
+     parser.add_argument(&#39;--reproduction&#39;, required=True, help=&#39;Path to reproduction PDF&#39;)
+     parser.add_argument(&#39;--dpi&#39;, type=int, default=300, help=&#39;DPI for PDF conversion (default: 300)&#39;)
+     parser.add_argument(&#39;--output&#39;, default=&#39;brightness_analysis&#39;, help=&#39;Output directory (default: brightness_analysis)&#39;)
+     
+     args = parser.parse_args()
+     
+     # Validate input files
+     if not os.path.exists(args.original):
+         print(f&quot;Error: Original file not found: {args.original}&quot;)
+         sys.exit(1)
+     
+     if not os.path.exists(args.reproduction):
+         print(f&quot;Error: Reproduction file not found: {args.reproduction}&quot;)
+         sys.exit(1)
+     
+     try:
+         # Create analyzer and run analysis
+         analyzer = BrightnessAnalyzer(dpi=args.dpi, output_dir=args.output)
+         report = analyzer.analyze_document(args.original, args.reproduction)
+         analyzer.print_summary(report)
+         
+     except Exception as e:
+         print(f&quot;Error during analysis: {e}&quot;)
+         sys.exit(1)
+ 
+ 
+ if __name__ == &quot;__main__&quot;:
+     main()
```

</details>

</tool-use>

---

Now let's create a test script to run brightness analysis on your three test cases:

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **code_edit** • Edit file: scanner/test_brightness.py</summary>

**Chunk 1**
Lines added: 114, lines removed: 1

```diff
@@ -1,1 +1,114 @@
- 
+ #!/usr/bin/env python3
+ &quot;&quot;&quot;
+ Test script for brightness analysis on microfilm samples.
+ 
+ Tests three cases:
+ - OK: Similar brightness, no faults
+ - NG: Wrong scanner settings, no faults  
+ - BK: Similar brightness, faulty (scratches)
+ &quot;&quot;&quot;
+ 
+ import os
+ import sys
+ from pathlib import Path
+ from brightness_analyzer import BrightnessAnalyzer
+ 
+ 
+ def main():
+     &quot;&quot;&quot;Run brightness analysis on all test cases.&quot;&quot;&quot;
+     
+     # Test file paths
+     test_dir = Path(&quot;test&quot;)
+     original_pdf = test_dir / &quot;orig&quot; / &quot;1427004500479387.pdf&quot;
+     
+     test_cases = {
+         &quot;OK&quot;: test_dir / &quot;scan&quot; / &quot;1427004500479387_SCAN_OK.pdf&quot;,
+         &quot;NG&quot;: test_dir / &quot;scan&quot; / &quot;1427004500479387_SCAN_NG.pdf&quot;, 
+         &quot;BK&quot;: test_dir / &quot;scan&quot; / &quot;1427004500479387_SCAN_BK.pdf&quot;
+     }
+     
+     # Verify files exist
+     if not original_pdf.exists():
+         print(f&quot;Error: Original file not found: {original_pdf}&quot;)
+         sys.exit(1)
+     
+     for case_name, case_path in test_cases.items():
+         if not case_path.exists():
+             print(f&quot;Warning: Test case {case_name} not found: {case_path}&quot;)
+     
+     # Run analysis for each test case
+     results = {}
+     
+     for case_name, case_path in test_cases.items():
+         if not case_path.exists():
+             continue
+             
+         print(f&quot;\n{&#39;=&#39;*60}&quot;)
+         print(f&quot;ANALYZING CASE: {case_name}&quot;)
+         print(f&quot;{&#39;=&#39;*60}&quot;)
+         
+         try:
+             # Create analyzer with case-specific output directory
+             output_dir = f&quot;brightness_analysis_{case_name.lower()}&quot;
+             analyzer = BrightnessAnalyzer(dpi=300, output_dir=output_dir)
+             
+             # Run analysis
+             report = analyzer.analyze_document(str(original_pdf), str(case_path))
+             analyzer.print_summary(report)
+             
+             # Store results for comparison
+             results[case_name] = report[&#39;overall_assessment&#39;]
+             
+         except Exception as e:
+             print(f&quot;Error analyzing case {case_name}: {e}&quot;)
+             continue
+     
+     # Print comparative summary
+     print(f&quot;\n{&#39;=&#39;*80}&quot;)
+     print(&quot;COMPARATIVE BRIGHTNESS ANALYSIS SUMMARY&quot;)
+     print(f&quot;{&#39;=&#39;*80}&quot;)
+     
+     print(f&quot;{&#39;Case&#39;:&lt;8} {&#39;Quality Score&#39;:&lt;15} {&#39;Issues&#39;:&lt;50}&quot;)
+     print(f&quot;{&#39;-&#39;*8} {&#39;-&#39;*15} {&#39;-&#39;*50}&quot;)
+     
+     for case_name, assessment in results.items():
+         quality_score = f&quot;{assessment[&#39;average_quality_score&#39;]:.3f}&quot;
+         issues = &quot;, &quot;.join(assessment[&#39;unique_issues&#39;][:2]) if assessment[&#39;unique_issues&#39;] else &quot;None&quot;
+         if len(issues) &gt; 47:
+             issues = issues[:44] + &quot;...&quot;
+         print(f&quot;{case_name:&lt;8} {quality_score:&lt;15} {issues:&lt;50}&quot;)
+     
+     print(f&quot;\n{&#39;=&#39;*80}&quot;)
+     print(&quot;ANALYSIS INTERPRETATION:&quot;)
+     print(f&quot;{&#39;=&#39;*80}&quot;)
+     
+     # Provide interpretation based on results
+     if &#39;OK&#39; in results and &#39;NG&#39; in results:
+         ok_score = results[&#39;OK&#39;][&#39;average_quality_score&#39;]
+         ng_score = results[&#39;NG&#39;][&#39;average_quality_score&#39;]
+         
+         print(f&quot;OK vs NG Comparison:&quot;)
+         print(f&quot;  OK Quality Score: {ok_score:.3f}&quot;)
+         print(f&quot;  NG Quality Score: {ng_score:.3f}&quot;)
+         print(f&quot;  Difference: {abs(ok_score - ng_score):.3f}&quot;)
+         
+         if ng_score &lt; ok_score - 0.1:
+             print(f&quot;  → NG case shows significantly lower quality (likely scanner settings issue)&quot;)
+         elif ng_score &gt; ok_score + 0.1:
+             print(f&quot;  → NG case shows higher brightness (possible overexposure in scanning)&quot;)
+         else:
+             print(f&quot;  → Both cases show similar quality scores&quot;)
+     
+     if &#39;BK&#39; in results:
+         bk_score = results[&#39;BK&#39;][&#39;average_quality_score&#39;]
+         print(f&quot;\nBK (Faulty) Case:&quot;)
+         print(f&quot;  Quality Score: {bk_score:.3f}&quot;)
+         if bk_score &lt; 0.7:
+             print(f&quot;  → Low quality score suggests physical defects detected&quot;)
+         print(f&quot;  Issues: {&#39;, &#39;.join(results[&#39;BK&#39;][&#39;unique_issues&#39;])}&quot;)
+     
+     print(f&quot;\nDetailed results saved in respective brightness_analysis_* directories&quot;)
+ 
+ 
+ if __name__ == &quot;__main__&quot;:
+     main()
```

</details>

</tool-use>

---

<tool-use data-tool-type="generic" data-tool-name="todo_write">
<details><summary>Tool use: **todo_write** • Todo List</summary>

- [x] Analyze current system architecture and identify improvement areas
- [ ] Design comprehensive AOI framework with configurable inspection modules
- [ ] Plan GUI interface with preview windows and real-time visualization
- [ ] Design advanced defect detection algorithms (positioning, content, quality)
- [ ] Plan flexible configuration system for thresholds and inspection parameters
- [x] Design image processing pipeline for microfilm defect detection
- [ ] Implement overexposure/underexposure detection algorithms
- [ ] Implement scratch and artifact detection
- [ ] Implement OCR-based readability assessment
- [x] Create brightness/contrast analysis script for test PDFs

</details>
</tool-use>

---

Perfect! I've created a comprehensive brightness analysis system. Here's how it works:

## **Brightness Analysis Strategy Explained**

### **Why This Approach?**

1. **Edge Region Analysis**: Measures document margins (5% from each edge) where there's typically only background paper - no text content to interfere with brightness measurements.

2. **White Region Detection**: Automatically finds large white/background areas within the document content using:
   - Thresholding to identify white pixels (>200 intensity)
   - Morphological operations to remove noise
   - Connected component analysis to find large regions
   - Size filtering to ignore small white spots

3. **Multi-Zone Sampling**: Analyzes:
   - **Corners**: Least likely to have content
   - **Edges**: Top, bottom, left, right margins  
   - **Interior White Regions**: Large background areas between text blocks
   - **Overall Statistics**: For general comparison

### **What It Detects**

- **Overexposure**: Reproduction brighter than original (ratio > 1.15)
- **Underexposure**: Reproduction darker than original (ratio < 0.85)  
- **Poor Contrast**: Lost contrast compared to original (ratio < 0.8)
- **Background Shift**: Changes in paper background brightness

### **Key Features**

1. **Visual Analysis**: Shows edge regions, white regions, and histograms
2. **Statistical Comparison**: Quantitative metrics for each region type
3. **Quality Scoring**: Automated pass/fail assessment
4. **Detailed Reporting**: JSON reports with all measurements

## **How to Run the Tests**

---

<tool-use data-tool-type="bash" data-tool-name="run_terminal_cmd">
<details><summary>Tool use: **command** • Run command: cd scanner && python test_brightness.py</summary>

```bash
cd scanner && python test_brightness.py
```
</details>
</tool-use>

---

_**User (2025-11-08 19:36Z)**_

place it under @brightness 

---

_**Agent (model claude-4-sonnet, mode Agent)**_

<tool-use data-tool-type="generic" data-tool-name="list_dir">
<details>
            <summary>Tool use: **list_directory** • Listed directory undefined • **0** results</summary>
        
No results found
</details>
</tool-use>

---

I'll move the brightness analysis files to the AOI/brightness directory:

---

<tool-use data-tool-type="bash" data-tool-name="run_terminal_cmd">
<details><summary>Tool use: **command** • Run command: cd scanner && move brightness_analyzer.py AOI\brightness\ && move test_brightness.py AOI\brightness\</summary>

```bash
cd scanner && move brightness_analyzer.py AOI\brightness\ && move test_brightness.py AOI\brightness\
```
</details>
</tool-use>

---

Perfect! The brightness analysis files are now organized under the AOI/brightness directory. Let me verify they're in the right place:

---

<tool-use data-tool-type="generic" data-tool-name="list_dir">
<details>
            <summary>Tool use: **list_directory** • Listed directory undefined • **0** results</summary>
        
No results found
</details>
</tool-use>

---

Great! Now let's update the test script to work from the new location:

---

<tool-use data-tool-type="write" data-tool-name="search_replace">

<details><summary>Tool use: **code_edit** • Edit file: scanner/AOI/brightness/test_brightness.py</summary>

**Chunk 1**
Lines added: 2, lines removed: 2

```diff
@@ -18,6 +18,6 @@
      &quot;&quot;&quot;Run brightness analysis on all test cases.&quot;&quot;&quot;
      
-     # Test file paths
-     test_dir = Path(&quot;test&quot;)
+     # Test file paths (relative to scanner root)
+     test_dir = Path(&quot;../../test&quot;)
      original_pdf = test_dir / &quot;orig&quot; / &quot;1427004500479387.pdf&quot;
      
```

</details>

</tool-use>

---

Now the brightness analysis tools are properly organized under `scanner/AOI/brightness/`. 

## **File Structure:**
```
scanner/
├── AOI/
│   └── brightness/
│       ├── brightness_analyzer.py  # Main brightness analysis tool
│       └── test_brightness.py      # Test script for your 3 cases
├── test/
│   ├── orig/
│   │   └── 1427004500479387.pdf
│   └── scan/
│       ├── 1427004500479387_SCAN_OK.pdf
│       ├── 1427004500479387_SCAN_NG.pdf
│       └── 1427004500479387_SCAN_BK.pdf
└── ...
```

## **To run the brightness tests:**

```bash
cd scanner/AOI/brightness
python test_brightness.py
```

This will analyze all three cases (OK, NG, BK) and create detailed brightness comparison reports with visualizations showing:

1. **Edge region analysis** (document margins)
2. **White region detection** (background areas)  
3. **Histogram comparisons**
4. **Quality scoring** with specific issue detection
5. **Visual overlays** showing analyzed regions

The system will detect issues like:
- Scanner settings problems (brightness/contrast differences)
- Physical defects (scratches, spots)
- Exposure issues (over/under exposed film)

Would you like to run the test now or would you prefer to modify any parameters first?

---

_**User (2025-11-08 20:38Z)**_

create a new directory for the orientation finder.



how do you want to do that? since we have pdfs for both, do we want to orient the pdfs first or the images?

---

_**Agent (model claude-4-sonnet, mode Agent)**_

<tool-use data-tool-type="generic" data-tool-name="todo_write">
<details><summary>Tool use: **todo_write** • Todo List</summary>

- [x] Analyze current system architecture and identify improvement areas
- [ ] Design comprehensive AOI framework with configurable inspection modules
- [ ] Plan GUI interface with preview windows and real-time visualization
- [ ] Design advanced defect detection algorithms (positioning, content, quality)
- [ ] Plan flexible configuration system for thresholds and inspection parameters
- [x] Design image processing pipeline for microfilm defect detection
- [ ] Implement overexposure/underexposure detection algorithms
- [ ] Implement scratch and artifact detection
- [ ] Implement OCR-based readability assessment
- [x] Create brightness/contrast analysis script for test PDFs
- [-] Create orientation detection module for PDF alignment

</details>
</tool-use>

---

<tool-use data-tool-type="bash" data-tool-name="run_terminal_cmd">
<details><summary>Tool use: **command** • Run command: cd scanner/AOI && mkdir orientation</summary>

```bash
cd scanner/AOI && mkdir orientation
```

```
At C:\Users\RAVEN\AppData\Local\Temp\ps-script-42ac9bb5-760a-4009-a761-210a6042e9b5.ps1:7 char:16
+ cd scanner/AOI &amp;&amp; mkdir orientation
+                ~~
The token &#39;&amp;&amp;&#39; is not a valid statement separator in this version.
    + CategoryInfo          : ParserError: (:) [], ParentContainsErrorRecordException
    + FullyQualifiedErrorId : InvalidEndOfLine
 

```
</details>
</tool-use>

---

<tool-use data-tool-type="bash" data-tool-name="run_terminal_cmd">
<details><summary>Tool use: **command** • Run command: cd scanner/AOI; mkdir orientation</summary>

```bash
cd scanner/AOI; mkdir orientation
```

```


    Directory: C:\Users\RAVEN\Desktop\micro\scanner\AOI


Mode                 LastWriteTime         Length Name                                                                 
----                 -------------         ------ ----                                                                 
d-----         11/8/2025   9:41 PM                orientation                                                          

```
</details>
</tool-use>

---

Great! Now for the orientation detection strategy - this is a crucial design decision. Let me explain the options:

## **PDF vs Image Orientation Detection**

### **Option 1: PDF-Level Orientation (Recommended)**
```python
# Detect orientation at PDF level, then convert with correct orientation
def detect_pdf_orientation(pdf_path):
    # Convert first page to image
    # Detect orientation 
    # Apply rotation to entire PDF
    # Then convert all pages to properly oriented images
```

**Advantages:**
- More efficient (detect once, apply to all pages)
- Preserves PDF metadata and structure
- Consistent orientation across all pages
- Can leverage PDF's built-in rotation capabilities

### **Option 2: Image-Level Orientation**
```python
# Convert PDF to images first, then detect/correct orientation per image
def detect_image_orientation(image):
    # Analyze each converted image individually
    # Rotate each image as needed
```

**Advantages:**
- More flexible (can handle mixed orientations within same document)
- Direct image processing control
- Can handle page-specific rotation issues

## **My Recommendation: Hybrid Approach**

For microfilm AOI, I suggest a **two-stage approach**:

1. **PDF-Level Coarse Detection**: Detect major rotation (0°, 90°, 180°, 270°) at PDF level
2. **Image-Level Fine Tuning**: After conversion, apply fine angle corrections (±10°) at image level

Here's why this works best for microfilm:

```python
class OrientationDetector:
    """
    Two-stage orientation detection for microfilm documents
    """
    
    def detect_pdf_orientation(self, original_pdf, microfilm_pdf):
        """
        Stage 1: Detect coarse rotation at PDF level
        - Fast detection using first page or representative pages
        - Handles 90-degree increments (common in microfilm scanning)
        """
        
    def detect_fine_orientation(self, orig_image, film_image):
        """
        Stage 2: Fine-tune orientation at image level  
        - Handles small angle corrections (±10°)
        - Accounts for film/scanning imperfections
        """
```

## **Implementation Strategy**

---

Cancelled

---

_**User (2025-11-08 20:38Z)**_

or how about on image level?



because its not always clear which orientation the original and the reproduction is or will be.



can you create a solid conversion module since this will always be required?



---

_**Agent (model claude-4-sonnet, mode Agent)**_

You're absolutely right! Image-level orientation detection is more robust because we can't predict how either the original or reproduction PDFs are oriented. Let me create a solid conversion module that handles this properly.

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **code_edit** • Edit file: scanner/AOI/orientation/pdf_converter.py</summary>

**Chunk 1**
Lines added: 444, lines removed: 1

```diff
@@ -1,1 +1,444 @@
- 
+ #!/usr/bin/env python3
+ &quot;&quot;&quot;
+ PDF to Image Conversion Module
+ 
+ Robust PDF conversion with orientation handling for microfilm AOI analysis.
+ Handles both original and reproduction PDFs regardless of their initial orientation.
+ &quot;&quot;&quot;
+ 
+ import os
+ import sys
+ from pathlib import Path
+ from typing import List, Tuple, Dict, Any, Optional
+ import numpy as np
+ import cv2
+ from pdf2image import convert_from_path
+ import fitz  # PyMuPDF
+ from PIL import Image
+ import io
+ import json
+ from datetime import datetime
+ 
+ 
+ class PDFConverter:
+     &quot;&quot;&quot;
+     Robust PDF to image converter with orientation detection capabilities.
+     &quot;&quot;&quot;
+     
+     def __init__(self, dpi: int = 300, output_dir: str = &quot;conversion_output&quot;):
+         self.dpi = dpi
+         self.output_dir = Path(output_dir)
+         self.output_dir.mkdir(exist_ok=True)
+         
+         # Conversion settings
+         self.grayscale = True
+         self.normalize_size = True
+         self.save_intermediate = True  # Save converted images for debugging
+         
+     def convert_pdf_to_images(self, pdf_path: str, method: str = &quot;pdf2image&quot;) -&gt; List[np.ndarray]:
+         &quot;&quot;&quot;
+         Convert PDF to list of grayscale numpy arrays.
+         
+         Args:
+             pdf_path: Path to PDF file
+             method: Conversion method (&quot;pdf2image&quot; or &quot;pymupdf&quot;)
+             
+         Returns:
+             List of grayscale numpy arrays (one per page)
+         &quot;&quot;&quot;
+         print(f&quot;Converting PDF: {pdf_path}&quot;)
+         print(f&quot;Method: {method}, DPI: {self.dpi}&quot;)
+         
+         if method == &quot;pdf2image&quot;:
+             return self._convert_with_pdf2image(pdf_path)
+         elif method == &quot;pymupdf&quot;:
+             return self._convert_with_pymupdf(pdf_path)
+         else:
+             raise ValueError(f&quot;Unknown conversion method: {method}&quot;)
+     
+     def _convert_with_pdf2image(self, pdf_path: str) -&gt; List[np.ndarray]:
+         &quot;&quot;&quot;Convert using pdf2image library.&quot;&quot;&quot;
+         try:
+             # Convert PDF to PIL images
+             pil_images = convert_from_path(pdf_path, dpi=self.dpi)
+             
+             images = []
+             for i, pil_img in enumerate(pil_images):
+                 # Convert to numpy array
+                 img_array = np.array(pil_img)
+                 
+                 # Convert to grayscale if needed
+                 if len(img_array.shape) == 3:
+                     if self.grayscale:
+                         gray_img = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
+                     else:
+                         gray_img = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)
+                 else:
+                     gray_img = img_array
+                 
+                 images.append(gray_img)
+                 
+                 # Save intermediate image if requested
+                 if self.save_intermediate:
+                     self._save_intermediate_image(gray_img, pdf_path, i, &quot;pdf2image&quot;)
+             
+             print(f&quot;Successfully converted {len(images)} pages using pdf2image&quot;)
+             return images
+             
+         except Exception as e:
+             print(f&quot;Error with pdf2image conversion: {e}&quot;)
+             return []
+     
+     def _convert_with_pymupdf(self, pdf_path: str) -&gt; List[np.ndarray]:
+         &quot;&quot;&quot;Convert using PyMuPDF library.&quot;&quot;&quot;
+         try:
+             pdf_doc = fitz.open(pdf_path)
+             images = []
+             
+             # Calculate scale factor for desired DPI
+             scale = self.dpi / 72.0
+             mat = fitz.Matrix(scale, scale)
+             
+             for page_num in range(len(pdf_doc)):
+                 page = pdf_doc[page_num]
+                 
+                 # Render page to pixmap
+                 pix = page.get_pixmap(matrix=mat)
+                 
+                 # Convert to PIL Image
+                 img_data = pix.tobytes(&quot;ppm&quot;)
+                 pil_img = Image.open(io.BytesIO(img_data))
+                 
+                 # Convert to numpy array
+                 img_array = np.array(pil_img)
+                 
+                 # Convert to grayscale if needed
+                 if len(img_array.shape) == 3:
+                     if self.grayscale:
+                         gray_img = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
+                     else:
+                         gray_img = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)
+                 else:
+                     gray_img = img_array
+                 
+                 images.append(gray_img)
+                 
+                 # Save intermediate image if requested
+                 if self.save_intermediate:
+                     self._save_intermediate_image(gray_img, pdf_path, page_num, &quot;pymupdf&quot;)
+             
+             pdf_doc.close()
+             print(f&quot;Successfully converted {len(images)} pages using PyMuPDF&quot;)
+             return images
+             
+         except Exception as e:
+             print(f&quot;Error with PyMuPDF conversion: {e}&quot;)
+             return []
+     
+     def _save_intermediate_image(self, img: np.ndarray, pdf_path: str, page_num: int, method: str):
+         &quot;&quot;&quot;Save intermediate converted image for debugging.&quot;&quot;&quot;
+         pdf_name = Path(pdf_path).stem
+         filename = f&quot;{pdf_name}_page_{page_num+1:03d}_{method}.png&quot;
+         output_path = self.output_dir / filename
+         
+         cv2.imwrite(str(output_path), img)
+     
+     def convert_document_pair(self, original_pdf: str, reproduction_pdf: str, 
+                             method: str = &quot;pdf2image&quot;) -&gt; Tuple[List[np.ndarray], List[np.ndarray], Dict[str, Any]]:
+         &quot;&quot;&quot;
+         Convert both original and reproduction PDFs to images.
+         
+         Returns:
+             Tuple of (original_images, reproduction_images, conversion_info)
+         &quot;&quot;&quot;
+         print(f&quot;\n{&#39;=&#39;*60}&quot;)
+         print(&quot;CONVERTING DOCUMENT PAIR&quot;)
+         print(f&quot;{&#39;=&#39;*60}&quot;)
+         
+         conversion_info = {
+             &#39;timestamp&#39;: datetime.now().isoformat(),
+             &#39;original_pdf&#39;: original_pdf,
+             &#39;reproduction_pdf&#39;: reproduction_pdf,
+             &#39;method&#39;: method,
+             &#39;dpi&#39;: self.dpi,
+             &#39;grayscale&#39;: self.grayscale
+         }
+         
+         # Convert original
+         print(f&quot;\n1. Converting Original Document:&quot;)
+         original_images = self.convert_pdf_to_images(original_pdf, method)
+         conversion_info[&#39;original_pages&#39;] = len(original_images)
+         
+         if original_images:
+             conversion_info[&#39;original_dimensions&#39;] = {
+                 &#39;height&#39;: int(original_images[0].shape[0]),
+                 &#39;width&#39;: int(original_images[0].shape[1])
+             }
+         
+         # Convert reproduction
+         print(f&quot;\n2. Converting Reproduction Document:&quot;)
+         reproduction_images = self.convert_pdf_to_images(reproduction_pdf, method)
+         conversion_info[&#39;reproduction_pages&#39;] = len(reproduction_images)
+         
+         if reproduction_images:
+             conversion_info[&#39;reproduction_dimensions&#39;] = {
+                 &#39;height&#39;: int(reproduction_images[0].shape[0]),
+                 &#39;width&#39;: int(reproduction_images[0].shape[1])
+             }
+         
+         # Check for dimension mismatches
+         if original_images and reproduction_images:
+             orig_shape = original_images[0].shape
+             repro_shape = reproduction_images[0].shape
+             
+             if orig_shape != repro_shape:
+                 print(f&quot;\nWarning: Dimension mismatch detected!&quot;)
+                 print(f&quot;  Original: {orig_shape}&quot;)
+                 print(f&quot;  Reproduction: {repro_shape}&quot;)
+                 conversion_info[&#39;dimension_mismatch&#39;] = True
+                 conversion_info[&#39;size_normalization_needed&#39;] = True
+             else:
+                 conversion_info[&#39;dimension_mismatch&#39;] = False
+                 conversion_info[&#39;size_normalization_needed&#39;] = False
+         
+         # Save conversion report
+         report_path = self.output_dir / &quot;conversion_report.json&quot;
+         with open(report_path, &#39;w&#39;) as f:
+             json.dump(conversion_info, f, indent=2)
+         
+         print(f&quot;\nConversion complete!&quot;)
+         print(f&quot;Original pages: {len(original_images)}&quot;)
+         print(f&quot;Reproduction pages: {len(reproduction_images)}&quot;)
+         print(f&quot;Report saved: {report_path}&quot;)
+         
+         return original_images, reproduction_images, conversion_info
+     
+     def normalize_image_sizes(self, images1: List[np.ndarray], images2: List[np.ndarray], 
+                             target_size: Optional[Tuple[int, int]] = None) -&gt; Tuple[List[np.ndarray], List[np.ndarray]]:
+         &quot;&quot;&quot;
+         Normalize image sizes between two sets of images.
+         
+         Args:
+             images1: First set of images
+             images2: Second set of images  
+             target_size: Optional target size (width, height). If None, uses size of first image in images1
+             
+         Returns:
+             Tuple of normalized image lists
+         &quot;&quot;&quot;
+         if not images1 or not images2:
+             return images1, images2
+         
+         # Determine target size
+         if target_size is None:
+             target_size = (images1[0].shape[1], images1[0].shape[0])  # (width, height)
+         
+         print(f&quot;Normalizing images to size: {target_size}&quot;)
+         
+         # Resize images1
+         normalized_images1 = []
+         for img in images1:
+             if img.shape[:2] != (target_size[1], target_size[0]):  # OpenCV uses (height, width)
+                 resized = cv2.resize(img, target_size)
+                 normalized_images1.append(resized)
+             else:
+                 normalized_images1.append(img.copy())
+         
+         # Resize images2
+         normalized_images2 = []
+         for img in images2:
+             if img.shape[:2] != (target_size[1], target_size[0]):  # OpenCV uses (height, width)
+                 resized = cv2.resize(img, target_size)
+                 normalized_images2.append(resized)
+             else:
+                 normalized_images2.append(img.copy())
+         
+         return normalized_images1, normalized_images2
+     
+     def get_conversion_quality_metrics(self, images: List[np.ndarray]) -&gt; Dict[str, Any]:
+         &quot;&quot;&quot;
+         Analyze quality metrics of converted images.
+         
+         Returns:
+             Dictionary with quality metrics
+         &quot;&quot;&quot;
+         if not images:
+             return {}
+         
+         metrics = {
+             &#39;total_pages&#39;: len(images),
+             &#39;dimensions&#39;: {
+                 &#39;height&#39;: int(images[0].shape[0]),
+                 &#39;width&#39;: int(images[0].shape[1])
+             },
+             &#39;per_page_stats&#39;: []
+         }
+         
+         for i, img in enumerate(images):
+             page_stats = {
+                 &#39;page&#39;: i + 1,
+                 &#39;mean_brightness&#39;: float(np.mean(img)),
+                 &#39;std_brightness&#39;: float(np.std(img)),
+                 &#39;min_brightness&#39;: float(np.min(img)),
+                 &#39;max_brightness&#39;: float(np.max(img)),
+                 &#39;dynamic_range&#39;: float(np.max(img) - np.min(img))
+             }
+             metrics[&#39;per_page_stats&#39;].append(page_stats)
+         
+         # Overall statistics
+         all_means = [p[&#39;mean_brightness&#39;] for p in metrics[&#39;per_page_stats&#39;]]
+         all_stds = [p[&#39;std_brightness&#39;] for p in metrics[&#39;per_page_stats&#39;]]
+         all_ranges = [p[&#39;dynamic_range&#39;] for p in metrics[&#39;per_page_stats&#39;]]
+         
+         metrics[&#39;overall_stats&#39;] = {
+             &#39;avg_brightness&#39;: float(np.mean(all_means)),
+             &#39;avg_contrast&#39;: float(np.mean(all_stds)),
+             &#39;avg_dynamic_range&#39;: float(np.mean(all_ranges)),
+             &#39;brightness_consistency&#39;: float(np.std(all_means)),  # Lower is more consistent
+             &#39;contrast_consistency&#39;: float(np.std(all_stds))
+         }
+         
+         return metrics
+ 
+ 
+ class ConversionTester:
+     &quot;&quot;&quot;Test conversion with different methods and settings.&quot;&quot;&quot;
+     
+     def __init__(self, output_base_dir: str = &quot;conversion_tests&quot;):
+         self.output_base_dir = Path(output_base_dir)
+         self.output_base_dir.mkdir(exist_ok=True)
+     
+     def test_conversion_methods(self, original_pdf: str, reproduction_pdf: str) -&gt; Dict[str, Any]:
+         &quot;&quot;&quot;
+         Test different conversion methods and compare results.
+         &quot;&quot;&quot;
+         print(f&quot;\n{&#39;=&#39;*70}&quot;)
+         print(&quot;TESTING CONVERSION METHODS&quot;)
+         print(f&quot;{&#39;=&#39;*70}&quot;)
+         
+         methods = [&quot;pdf2image&quot;, &quot;pymupdf&quot;]
+         results = {}
+         
+         for method in methods:
+             print(f&quot;\nTesting method: {method}&quot;)
+             
+             # Create method-specific output directory
+             method_dir = self.output_base_dir / f&quot;test_{method}&quot;
+             
+             try:
+                 converter = PDFConverter(dpi=300, output_dir=method_dir)
+                 orig_images, repro_images, conversion_info = converter.convert_document_pair(
+                     original_pdf, reproduction_pdf, method=method
+                 )
+                 
+                 # Get quality metrics
+                 orig_metrics = converter.get_conversion_quality_metrics(orig_images)
+                 repro_metrics = converter.get_conversion_quality_metrics(repro_images)
+                 
+                 results[method] = {
+                     &#39;success&#39;: True,
+                     &#39;conversion_info&#39;: conversion_info,
+                     &#39;original_metrics&#39;: orig_metrics,
+                     &#39;reproduction_metrics&#39;: repro_metrics,
+                     &#39;output_directory&#39;: str(method_dir)
+                 }
+                 
+                 print(f&quot;  ✓ Success: {len(orig_images)} original, {len(repro_images)} reproduction pages&quot;)
+                 
+             except Exception as e:
+                 print(f&quot;  ✗ Failed: {e}&quot;)
+                 results[method] = {
+                     &#39;success&#39;: False,
+                     &#39;error&#39;: str(e)
+                 }
+         
+         # Save comparison report
+         report_path = self.output_base_dir / &quot;method_comparison_report.json&quot;
+         with open(report_path, &#39;w&#39;) as f:
+             json.dump(results, f, indent=2)
+         
+         print(f&quot;\nMethod comparison report saved: {report_path}&quot;)
+         return results
+     
+     def print_comparison_summary(self, results: Dict[str, Any]):
+         &quot;&quot;&quot;Print summary of method comparison.&quot;&quot;&quot;
+         
+         print(f&quot;\n{&#39;=&#39;*70}&quot;)
+         print(&quot;CONVERSION METHOD COMPARISON SUMMARY&quot;)
+         print(f&quot;{&#39;=&#39;*70}&quot;)
+         
+         for method, result in results.items():
+             print(f&quot;\nMethod: {method.upper()}&quot;)
+             print(&quot;-&quot; * 20)
+             
+             if result[&#39;success&#39;]:
+                 orig_metrics = result[&#39;original_metrics&#39;]
+                 repro_metrics = result[&#39;reproduction_metrics&#39;]
+                 
+                 print(f&quot;✓ Conversion successful&quot;)
+                 print(f&quot;  Original pages: {orig_metrics[&#39;total_pages&#39;]}&quot;)
+                 print(f&quot;  Reproduction pages: {repro_metrics[&#39;total_pages&#39;]}&quot;)
+                 print(f&quot;  Image dimensions: {orig_metrics[&#39;dimensions&#39;][&#39;width&#39;]}x{orig_metrics[&#39;dimensions&#39;][&#39;height&#39;]}&quot;)
+                 print(f&quot;  Avg brightness (orig): {orig_metrics[&#39;overall_stats&#39;][&#39;avg_brightness&#39;]:.1f}&quot;)
+                 print(f&quot;  Avg brightness (repro): {repro_metrics[&#39;overall_stats&#39;][&#39;avg_brightness&#39;]:.1f}&quot;)
+                 print(f&quot;  Output: {result[&#39;output_directory&#39;]}&quot;)
+             else:
+                 print(f&quot;✗ Conversion failed: {result[&#39;error&#39;]}&quot;)
+ 
+ 
+ def main():
+     &quot;&quot;&quot;Main function for testing conversion.&quot;&quot;&quot;
+     
+     import argparse
+     
+     parser = argparse.ArgumentParser(description=&#39;Test PDF to image conversion methods&#39;)
+     parser.add_argument(&#39;--original&#39;, required=True, help=&#39;Path to original PDF&#39;)
+     parser.add_argument(&#39;--reproduction&#39;, required=True, help=&#39;Path to reproduction PDF&#39;)
+     parser.add_argument(&#39;--method&#39;, default=&#39;pdf2image&#39;, choices=[&#39;pdf2image&#39;, &#39;pymupdf&#39;], 
+                        help=&#39;Conversion method to use&#39;)
+     parser.add_argument(&#39;--test-all&#39;, action=&#39;store_true&#39;, help=&#39;Test all conversion methods&#39;)
+     parser.add_argument(&#39;--dpi&#39;, type=int, default=300, help=&#39;DPI for conversion&#39;)
+     parser.add_argument(&#39;--output&#39;, default=&#39;conversion_output&#39;, help=&#39;Output directory&#39;)
+     
+     args = parser.parse_args()
+     
+     # Validate input files
+     if not os.path.exists(args.original):
+         print(f&quot;Error: Original file not found: {args.original}&quot;)
+         sys.exit(1)
+     
+     if not os.path.exists(args.reproduction):
+         print(f&quot;Error: Reproduction file not found: {args.reproduction}&quot;)
+         sys.exit(1)
+     
+     try:
+         if args.test_all:
+             # Test all methods
+             tester = ConversionTester(args.output)
+             results = tester.test_conversion_methods(args.original, args.reproduction)
+             tester.print_comparison_summary(results)
+         else:
+             # Single method conversion
+             converter = PDFConverter(dpi=args.dpi, output_dir=args.output)
+             orig_images, repro_images, conversion_info = converter.convert_document_pair(
+                 args.original, args.reproduction, method=args.method
+             )
+             
+             # Print summary
+             print(f&quot;\n{&#39;=&#39;*50}&quot;)
+             print(&quot;CONVERSION SUMMARY&quot;)
+             print(f&quot;{&#39;=&#39;*50}&quot;)
+             print(f&quot;Method: {args.method}&quot;)
+             print(f&quot;Original pages: {len(orig_images)}&quot;)
+             print(f&quot;Reproduction pages: {len(repro_images)}&quot;)
+             if orig_images:
+                 print(f&quot;Image dimensions: {orig_images[0].shape}&quot;)
+             print(f&quot;Output directory: {args.output}&quot;)
+         
+     except Exception as e:
+         print(f&quot;Error during conversion: {e}&quot;)
+         sys.exit(1)
+ 
+ 
+ if __name__ == &quot;__main__&quot;:
+     main()
```

</details>

</tool-use>

---

Now let's create the orientation detector that works on the converted images:

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **code_edit** • Edit file: scanner/AOI/orientation/orientation_detector.py</summary>

**Chunk 1**
Lines added: 606, lines removed: 1

```diff
@@ -1,1 +1,606 @@
- 
+ #!/usr/bin/env python3
+ &quot;&quot;&quot;
+ Orientation Detection Module
+ 
+ Detects and corrects orientation differences between original and reproduction images.
+ Works at the image level for maximum flexibility and accuracy.
+ &quot;&quot;&quot;
+ 
+ import os
+ import sys
+ from pathlib import Path
+ from typing import List, Tuple, Dict, Any, Optional
+ import numpy as np
+ import cv2
+ import matplotlib.pyplot as plt
+ import matplotlib.patches as patches
+ from skimage.metrics import structural_similarity as ssim
+ from skimage.feature import match_template
+ import json
+ from datetime import datetime
+ from pdf_converter import PDFConverter
+ 
+ 
+ class OrientationDetector:
+     &quot;&quot;&quot;
+     Detects orientation differences between original and reproduction images.
+     &quot;&quot;&quot;
+     
+     def __init__(self, output_dir: str = &quot;orientation_analysis&quot;):
+         self.output_dir = Path(output_dir)
+         self.output_dir.mkdir(exist_ok=True)
+         
+         # Detection parameters
+         self.test_rotations = [0, 90, 180, 270]  # Coarse rotations to test
+         self.fine_angle_range = 10  # ±10 degrees for fine adjustment
+         self.fine_angle_step = 0.5  # 0.5 degree increments
+         
+         # Image processing parameters
+         self.edge_detection_threshold = (50, 150)  # Canny edge detection thresholds
+         self.resize_for_speed = True  # Resize images for faster processing
+         self.max_dimension = 800  # Maximum dimension for speed optimization
+         
+         # Scoring parameters
+         self.ssim_weight = 0.7
+         self.correlation_weight = 0.3
+     
+     def preprocess_for_orientation(self, img: np.ndarray) -&gt; np.ndarray:
+         &quot;&quot;&quot;
+         Preprocess image for orientation detection.
+         
+         Args:
+             img: Input grayscale image
+             
+         Returns:
+             Preprocessed image optimized for orientation detection
+         &quot;&quot;&quot;
+         # Resize for speed if needed
+         if self.resize_for_speed:
+             h, w = img.shape
+             if max(h, w) &gt; self.max_dimension:
+                 scale = self.max_dimension / max(h, w)
+                 new_h, new_w = int(h * scale), int(w * scale)
+                 img = cv2.resize(img, (new_w, new_h))
+         
+         # Apply edge detection for better orientation matching
+         edges = cv2.Canny(img, self.edge_detection_threshold[0], self.edge_detection_threshold[1])
+         
+         # Optional: Apply morphological operations to strengthen edges
+         kernel = np.ones((2, 2), np.uint8)
+         edges = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)
+         
+         return edges
+     
+     def rotate_image(self, img: np.ndarray, angle: float) -&gt; np.ndarray:
+         &quot;&quot;&quot;
+         Rotate image by specified angle.
+         
+         Args:
+             img: Input image
+             angle: Rotation angle in degrees (positive = counterclockwise)
+             
+         Returns:
+             Rotated image
+         &quot;&quot;&quot;
+         if angle == 0:
+             return img.copy()
+         elif angle == 90:
+             return cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)
+         elif angle == 180:
+             return cv2.rotate(img, cv2.ROTATE_180)
+         elif angle == 270:
+             return cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)
+         else:
+             # Fine rotation using affine transformation
+             h, w = img.shape[:2]
+             center = (w // 2, h // 2)
+             rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)
+             
+             # Calculate new dimensions to avoid cropping
+             cos_angle = abs(rotation_matrix[0, 0])
+             sin_angle = abs(rotation_matrix[0, 1])
+             new_w = int((h * sin_angle) + (w * cos_angle))
+             new_h = int((h * cos_angle) + (w * sin_angle))
+             
+             # Adjust rotation matrix for new dimensions
+             rotation_matrix[0, 2] += (new_w / 2) - center[0]
+             rotation_matrix[1, 2] += (new_h / 2) - center[1]
+             
+             return cv2.warpAffine(img, rotation_matrix, (new_w, new_h))
+     
+     def calculate_orientation_score(self, img1: np.ndarray, img2: np.ndarray) -&gt; float:
+         &quot;&quot;&quot;
+         Calculate orientation matching score between two images.
+         
+         Args:
+             img1: Reference image (original)
+             img2: Test image (rotated reproduction)
+             
+         Returns:
+             Orientation score (higher = better match)
+         &quot;&quot;&quot;
+         # Ensure images are same size
+         if img1.shape != img2.shape:
+             img2 = cv2.resize(img2, (img1.shape[1], img1.shape[0]))
+         
+         # Calculate SSIM score
+         try:
+             ssim_score, _ = ssim(img1, img2, full=True)
+         except Exception:
+             ssim_score = 0.0
+         
+         # Calculate normalized cross-correlation
+         try:
+             # Normalize images
+             img1_norm = cv2.normalize(img1.astype(np.float32), None, 0, 1, cv2.NORM_MINMAX)
+             img2_norm = cv2.normalize(img2.astype(np.float32), None, 0, 1, cv2.NORM_MINMAX)
+             
+             # Calculate correlation coefficient
+             correlation = cv2.matchTemplate(img1_norm, img2_norm, cv2.TM_CCOEFF_NORMED)[0, 0]
+         except Exception:
+             correlation = 0.0
+         
+         # Weighted combination
+         combined_score = (self.ssim_weight * ssim_score + 
+                          self.correlation_weight * correlation)
+         
+         return combined_score
+     
+     def detect_coarse_orientation(self, original_img: np.ndarray, reproduction_img: np.ndarray) -&gt; Dict[str, Any]:
+         &quot;&quot;&quot;
+         Detect coarse orientation (0, 90, 180, 270 degrees).
+         
+         Args:
+             original_img: Original image
+             reproduction_img: Reproduction image
+             
+         Returns:
+             Dictionary with coarse orientation results
+         &quot;&quot;&quot;
+         print(&quot;Detecting coarse orientation (0°, 90°, 180°, 270°)...&quot;)
+         
+         # Preprocess images for orientation detection
+         orig_processed = self.preprocess_for_orientation(original_img)
+         repro_processed = self.preprocess_for_orientation(reproduction_img)
+         
+         best_score = -1
+         best_rotation = 0
+         rotation_scores = {}
+         
+         for rotation in self.test_rotations:
+             # Rotate reproduction image
+             rotated_repro = self.rotate_image(repro_processed, rotation)
+             
+             # Calculate orientation score
+             score = self.calculate_orientation_score(orig_processed, rotated_repro)
+             rotation_scores[rotation] = score
+             
+             print(f&quot;  Rotation {rotation:3d}°: Score = {score:.4f}&quot;)
+             
+             if score &gt; best_score:
+                 best_score = score
+                 best_rotation = rotation
+         
+         result = {
+             &#39;best_rotation&#39;: best_rotation,
+             &#39;best_score&#39;: best_score,
+             &#39;all_scores&#39;: rotation_scores,
+             &#39;confidence&#39;: best_score
+         }
+         
+         print(f&quot;  → Best coarse rotation: {best_rotation}° (score: {best_score:.4f})&quot;)
+         return result
+     
+     def detect_fine_orientation(self, original_img: np.ndarray, reproduction_img: np.ndarray, 
+                               coarse_rotation: int) -&gt; Dict[str, Any]:
+         &quot;&quot;&quot;
+         Detect fine orientation adjustment after coarse rotation.
+         
+         Args:
+             original_img: Original image
+             reproduction_img: Reproduction image
+             coarse_rotation: Coarse rotation angle to apply first
+             
+         Returns:
+             Dictionary with fine orientation results
+         &quot;&quot;&quot;
+         print(f&quot;Detecting fine orientation around {coarse_rotation}° (±{self.fine_angle_range}°)...&quot;)
+         
+         # Apply coarse rotation first
+         coarse_rotated = self.rotate_image(reproduction_img, coarse_rotation)
+         
+         # Preprocess images
+         orig_processed = self.preprocess_for_orientation(original_img)
+         repro_processed = self.preprocess_for_orientation(coarse_rotated)
+         
+         # Test fine angles
+         fine_angles = np.arange(-self.fine_angle_range, self.fine_angle_range + self.fine_angle_step, 
+                                self.fine_angle_step)
+         
+         best_score = -1
+         best_fine_angle = 0
+         angle_scores = {}
+         
+         for fine_angle in fine_angles:
+             # Apply fine rotation
+             fine_rotated = self.rotate_image(repro_processed, fine_angle)
+             
+             # Calculate orientation score
+             score = self.calculate_orientation_score(orig_processed, fine_rotated)
+             angle_scores[fine_angle] = score
+             
+             if score &gt; best_score:
+                 best_score = score
+                 best_fine_angle = fine_angle
+         
+         result = {
+             &#39;best_fine_angle&#39;: best_fine_angle,
+             &#39;best_score&#39;: best_score,
+             &#39;all_scores&#39;: angle_scores,
+             &#39;total_rotation&#39;: coarse_rotation + best_fine_angle,
+             &#39;confidence&#39;: best_score
+         }
+         
+         print(f&quot;  → Best fine adjustment: {best_fine_angle:.1f}° (score: {best_score:.4f})&quot;)
+         print(f&quot;  → Total rotation: {result[&#39;total_rotation&#39;]:.1f}°&quot;)
+         return result
+     
+     def apply_orientation_correction(self, img: np.ndarray, total_rotation: float) -&gt; np.ndarray:
+         &quot;&quot;&quot;
+         Apply the detected orientation correction to an image.
+         
+         Args:
+             img: Input image
+             total_rotation: Total rotation angle to apply
+             
+         Returns:
+             Corrected image
+         &quot;&quot;&quot;
+         return self.rotate_image(img, total_rotation)
+     
+     def detect_orientation(self, original_img: np.ndarray, reproduction_img: np.ndarray) -&gt; Dict[str, Any]:
+         &quot;&quot;&quot;
+         Complete orientation detection pipeline.
+         
+         Args:
+             original_img: Original image
+             reproduction_img: Reproduction image
+             
+         Returns:
+             Complete orientation detection results
+         &quot;&quot;&quot;
+         print(f&quot;\n{&#39;=&#39;*60}&quot;)
+         print(&quot;ORIENTATION DETECTION&quot;)
+         print(f&quot;{&#39;=&#39;*60}&quot;)
+         
+         # Step 1: Detect coarse orientation
+         coarse_result = self.detect_coarse_orientation(original_img, reproduction_img)
+         
+         # Step 2: Detect fine orientation
+         fine_result = self.detect_fine_orientation(original_img, reproduction_img, 
+                                                   coarse_result[&#39;best_rotation&#39;])
+         
+         # Combine results
+         orientation_result = {
+             &#39;timestamp&#39;: datetime.now().isoformat(),
+             &#39;coarse_detection&#39;: coarse_result,
+             &#39;fine_detection&#39;: fine_result,
+             &#39;final_rotation&#39;: fine_result[&#39;total_rotation&#39;],
+             &#39;final_confidence&#39;: fine_result[&#39;confidence&#39;],
+             &#39;detection_quality&#39;: self._assess_detection_quality(coarse_result, fine_result)
+         }
+         
+         return orientation_result
+     
+     def _assess_detection_quality(self, coarse_result: Dict, fine_result: Dict) -&gt; Dict[str, Any]:
+         &quot;&quot;&quot;Assess the quality of orientation detection.&quot;&quot;&quot;
+         
+         coarse_confidence = coarse_result[&#39;confidence&#39;]
+         fine_confidence = fine_result[&#39;confidence&#39;]
+         
+         # Check if there&#39;s a clear winner in coarse detection
+         scores = list(coarse_result[&#39;all_scores&#39;].values())
+         scores.sort(reverse=True)
+         coarse_clarity = scores[0] - scores[1] if len(scores) &gt; 1 else scores[0]
+         
+         quality_assessment = {
+             &#39;coarse_confidence&#39;: coarse_confidence,
+             &#39;fine_confidence&#39;: fine_confidence,
+             &#39;coarse_clarity&#39;: coarse_clarity,
+             &#39;overall_confidence&#39;: (coarse_confidence + fine_confidence) / 2
+         }
+         
+         # Determine quality level
+         if quality_assessment[&#39;overall_confidence&#39;] &gt; 0.8:
+             quality_assessment[&#39;quality_level&#39;] = &#39;HIGH&#39;
+         elif quality_assessment[&#39;overall_confidence&#39;] &gt; 0.6:
+             quality_assessment[&#39;quality_level&#39;] = &#39;MEDIUM&#39;
+         else:
+             quality_assessment[&#39;quality_level&#39;] = &#39;LOW&#39;
+         
+         # Add warnings
+         warnings = []
+         if coarse_clarity &lt; 0.1:
+             warnings.append(&quot;Low clarity in coarse detection - multiple orientations have similar scores&quot;)
+         if fine_confidence &lt; 0.5:
+             warnings.append(&quot;Low confidence in fine adjustment&quot;)
+         if abs(fine_result[&#39;best_fine_angle&#39;]) &gt; 5:
+             warnings.append(f&quot;Large fine adjustment needed ({fine_result[&#39;best_fine_angle&#39;]:.1f}°)&quot;)
+         
+         quality_assessment[&#39;warnings&#39;] = warnings
+         
+         return quality_assessment
+     
+     def create_orientation_visualization(self, original_img: np.ndarray, reproduction_img: np.ndarray,
+                                        orientation_result: Dict, page_num: int) -&gt; str:
+         &quot;&quot;&quot;
+         Create visualization of orientation detection results.
+         
+         Args:
+             original_img: Original image
+             reproduction_img: Reproduction image
+             orientation_result: Orientation detection results
+             page_num: Page number for naming
+             
+         Returns:
+             Path to saved visualization
+         &quot;&quot;&quot;
+         fig, axes = plt.subplots(2, 3, figsize=(18, 12))
+         fig.suptitle(f&#39;Page {page_num} Orientation Detection Results&#39;, fontsize=16, fontweight=&#39;bold&#39;)
+         
+         # Original image
+         axes[0, 0].imshow(original_img, cmap=&#39;gray&#39;)
+         axes[0, 0].set_title(&#39;Original Document&#39;)
+         axes[0, 0].axis(&#39;off&#39;)
+         
+         # Original reproduction (before correction)
+         axes[0, 1].imshow(reproduction_img, cmap=&#39;gray&#39;)
+         axes[0, 1].set_title(&#39;Reproduction (Original)&#39;)
+         axes[0, 1].axis(&#39;off&#39;)
+         
+         # Corrected reproduction
+         corrected_repro = self.apply_orientation_correction(reproduction_img, 
+                                                            orientation_result[&#39;final_rotation&#39;])
+         # Resize to match original for display
+         if corrected_repro.shape != original_img.shape:
+             corrected_repro = cv2.resize(corrected_repro, (original_img.shape[1], original_img.shape[0]))
+         
+         axes[0, 2].imshow(corrected_repro, cmap=&#39;gray&#39;)
+         axes[0, 2].set_title(f&#39;Corrected Reproduction\n(Rotated {orientation_result[&quot;final_rotation&quot;]:.1f}°)&#39;)
+         axes[0, 2].axis(&#39;off&#39;)
+         
+         # Coarse rotation scores
+         coarse_scores = orientation_result[&#39;coarse_detection&#39;][&#39;all_scores&#39;]
+         rotations = list(coarse_scores.keys())
+         scores = list(coarse_scores.values())
+         
+         axes[1, 0].bar(rotations, scores, color=[&#39;red&#39; if r == orientation_result[&#39;coarse_detection&#39;][&#39;best_rotation&#39;] 
+                                                else &#39;blue&#39; for r in rotations])
+         axes[1, 0].set_title(&#39;Coarse Rotation Scores&#39;)
+         axes[1, 0].set_xlabel(&#39;Rotation (degrees)&#39;)
+         axes[1, 0].set_ylabel(&#39;Score&#39;)
+         axes[1, 0].grid(True, alpha=0.3)
+         
+         # Fine angle scores (if available)
+         if &#39;all_scores&#39; in orientation_result[&#39;fine_detection&#39;]:
+             fine_scores = orientation_result[&#39;fine_detection&#39;][&#39;all_scores&#39;]
+             fine_angles = list(fine_scores.keys())
+             fine_score_values = list(fine_scores.values())
+             
+             axes[1, 1].plot(fine_angles, fine_score_values, &#39;b-&#39;, linewidth=2)
+             best_fine = orientation_result[&#39;fine_detection&#39;][&#39;best_fine_angle&#39;]
+             best_score = orientation_result[&#39;fine_detection&#39;][&#39;best_score&#39;]
+             axes[1, 1].plot(best_fine, best_score, &#39;ro&#39;, markersize=8, label=f&#39;Best: {best_fine:.1f}°&#39;)
+             axes[1, 1].set_title(&#39;Fine Angle Adjustment&#39;)
+             axes[1, 1].set_xlabel(&#39;Fine Angle (degrees)&#39;)
+             axes[1, 1].set_ylabel(&#39;Score&#39;)
+             axes[1, 1].grid(True, alpha=0.3)
+             axes[1, 1].legend()
+         
+         # Results summary
+         axes[1, 2].axis(&#39;off&#39;)
+         
+         quality = orientation_result[&#39;detection_quality&#39;]
+         summary_text = f&quot;&quot;&quot;
+ Orientation Detection Results:
+ 
+ Coarse Rotation: {orientation_result[&#39;coarse_detection&#39;][&#39;best_rotation&#39;]}°
+ Fine Adjustment: {orientation_result[&#39;fine_detection&#39;][&#39;best_fine_angle&#39;]:.1f}°
+ Total Rotation: {orientation_result[&#39;final_rotation&#39;]:.1f}°
+ 
+ Detection Quality: {quality[&#39;quality_level&#39;]}
+ Overall Confidence: {quality[&#39;overall_confidence&#39;]:.3f}
+ Coarse Confidence: {quality[&#39;coarse_confidence&#39;]:.3f}
+ Fine Confidence: {quality[&#39;fine_confidence&#39;]:.3f}
+ 
+ Warnings:
+ &quot;&quot;&quot;
+         
+         if quality[&#39;warnings&#39;]:
+             for warning in quality[&#39;warnings&#39;]:
+                 summary_text += f&quot;• {warning}\n&quot;
+         else:
+             summary_text += &quot;• No warnings\n&quot;
+         
+         axes[1, 2].text(0.05, 0.95, summary_text, fontsize=10, verticalalignment=&#39;top&#39;,
+                        fontfamily=&#39;monospace&#39;, transform=axes[1, 2].transAxes)
+         
+         # Add colored border based on quality
+         color_map = {&#39;HIGH&#39;: &#39;green&#39;, &#39;MEDIUM&#39;: &#39;orange&#39;, &#39;LOW&#39;: &#39;red&#39;}
+         color = color_map.get(quality[&#39;quality_level&#39;], &#39;gray&#39;)
+         rect = patches.Rectangle((0, 0), 1, 1, linewidth=4, edgecolor=color,
+                                facecolor=&#39;none&#39;, transform=axes[1, 2].transAxes)
+         axes[1, 2].add_patch(rect)
+         
+         plt.tight_layout()
+         
+         # Save visualization
+         output_path = self.output_dir / f&quot;page_{page_num:03d}_orientation_detection.png&quot;
+         plt.savefig(output_path, dpi=150, bbox_inches=&#39;tight&#39;)
+         plt.close()
+         
+         return str(output_path)
+     
+     def analyze_document_orientation(self, original_images: List[np.ndarray], 
+                                    reproduction_images: List[np.ndarray]) -&gt; Dict[str, Any]:
+         &quot;&quot;&quot;
+         Analyze orientation for entire document.
+         
+         Args:
+             original_images: List of original page images
+             reproduction_images: List of reproduction page images
+             
+         Returns:
+             Complete document orientation analysis
+         &quot;&quot;&quot;
+         print(f&quot;\n{&#39;=&#39;*70}&quot;)
+         print(&quot;DOCUMENT ORIENTATION ANALYSIS&quot;)
+         print(f&quot;{&#39;=&#39;*70}&quot;)
+         
+         min_pages = min(len(original_images), len(reproduction_images))
+         page_results = []
+         
+         for i in range(min_pages):
+             page_num = i + 1
+             print(f&quot;\nAnalyzing page {page_num}...&quot;)
+             
+             # Detect orientation for this page
+             orientation_result = self.detect_orientation(original_images[i], reproduction_images[i])
+             orientation_result[&#39;page&#39;] = page_num
+             
+             # Create visualization
+             viz_path = self.create_orientation_visualization(original_images[i], reproduction_images[i],
+                                                            orientation_result, page_num)
+             orientation_result[&#39;visualization_path&#39;] = viz_path
+             
+             page_results.append(orientation_result)
+         
+         # Analyze overall document orientation patterns
+         overall_analysis = self._analyze_document_patterns(page_results)
+         
+         # Generate report
+         report = {
+             &#39;timestamp&#39;: datetime.now().isoformat(),
+             &#39;analysis_type&#39;: &#39;orientation_detection&#39;,
+             &#39;total_pages&#39;: min_pages,
+             &#39;page_results&#39;: page_results,
+             &#39;overall_analysis&#39;: overall_analysis,
+             &#39;settings&#39;: {
+                 &#39;test_rotations&#39;: self.test_rotations,
+                 &#39;fine_angle_range&#39;: self.fine_angle_range,
+                 &#39;fine_angle_step&#39;: self.fine_angle_step,
+                 &#39;edge_detection_threshold&#39;: self.edge_detection_threshold
+             }
+         }
+         
+         # Save report
+         report_path = self.output_dir / &quot;orientation_analysis_report.json&quot;
+         with open(report_path, &#39;w&#39;) as f:
+             json.dump(report, f, indent=2)
+         
+         print(f&quot;\nOrientation analysis complete! Report saved to: {report_path}&quot;)
+         return report
+     
+     def _analyze_document_patterns(self, page_results: List[Dict]) -&gt; Dict[str, Any]:
+         &quot;&quot;&quot;Analyze orientation patterns across the document.&quot;&quot;&quot;
+         
+         rotations = [p[&#39;final_rotation&#39;] for p in page_results]
+         confidences = [p[&#39;final_confidence&#39;] for p in page_results]
+         
+         # Find most common rotation
+         from collections import Counter
+         coarse_rotations = [p[&#39;coarse_detection&#39;][&#39;best_rotation&#39;] for p in page_results]
+         rotation_counts = Counter(coarse_rotations)
+         most_common_rotation = rotation_counts.most_common(1)[0][0]
+         
+         analysis = {
+             &#39;most_common_coarse_rotation&#39;: most_common_rotation,
+             &#39;rotation_consistency&#39;: rotation_counts[most_common_rotation] / len(page_results),
+             &#39;average_fine_adjustment&#39;: float(np.mean([p[&#39;fine_detection&#39;][&#39;best_fine_angle&#39;] for p in page_results])),
+             &#39;average_confidence&#39;: float(np.mean(confidences)),
+             &#39;confidence_consistency&#39;: float(np.std(confidences)),
+             &#39;pages_with_warnings&#39;: sum(1 for p in page_results if p[&#39;detection_quality&#39;][&#39;warnings&#39;]),
+             &#39;quality_distribution&#39;: {
+                 &#39;HIGH&#39;: sum(1 for p in page_results if p[&#39;detection_quality&#39;][&#39;quality_level&#39;] == &#39;HIGH&#39;),
+                 &#39;MEDIUM&#39;: sum(1 for p in page_results if p[&#39;detection_quality&#39;][&#39;quality_level&#39;] == &#39;MEDIUM&#39;),
+                 &#39;LOW&#39;: sum(1 for p in page_results if p[&#39;detection_quality&#39;][&#39;quality_level&#39;] == &#39;LOW&#39;)
+             }
+         }
+         
+         # Recommendations
+         recommendations = []
+         if analysis[&#39;rotation_consistency&#39;] &lt; 0.8:
+             recommendations.append(&quot;Inconsistent rotation detected across pages - manual review recommended&quot;)
+         if analysis[&#39;average_confidence&#39;] &lt; 0.6:
+             recommendations.append(&quot;Low average confidence - consider manual orientation verification&quot;)
+         if abs(analysis[&#39;average_fine_adjustment&#39;]) &gt; 3:
+             recommendations.append(f&quot;Systematic skew detected ({analysis[&#39;average_fine_adjustment&#39;]:.1f}°) - check scanning alignment&quot;)
+         
+         analysis[&#39;recommendations&#39;] = recommendations
+         
+         return analysis
+ 
+ 
+ def main():
+     &quot;&quot;&quot;Main function for testing orientation detection.&quot;&quot;&quot;
+     
+     import argparse
+     
+     parser = argparse.ArgumentParser(description=&#39;Detect and correct document orientation&#39;)
+     parser.add_argument(&#39;--original&#39;, required=True, help=&#39;Path to original PDF&#39;)
+     parser.add_argument(&#39;--reproduction&#39;, required=True, help=&#39;Path to reproduction PDF&#39;)
+     parser.add_argument(&#39;--output&#39;, default=&#39;orientation_analysis&#39;, help=&#39;Output directory&#39;)
+     parser.add_argument(&#39;--dpi&#39;, type=int, default=300, help=&#39;DPI for PDF conversion&#39;)
+     
+     args = parser.parse_args()
+     
+     # Validate input files
+     if not os.path.exists(args.original):
+         print(f&quot;Error: Original file not found: {args.original}&quot;)
+         sys.exit(1)
+     
+     if not os.path.exists(args.reproduction):
+         print(f&quot;Error: Reproduction file not found: {args.reproduction}&quot;)
+         sys.exit(1)
+     
+     try:
+         # Convert PDFs to images
+         print(&quot;Converting PDFs to images...&quot;)
+         converter = PDFConverter(dpi=args.dpi, output_dir=f&quot;{args.output}/conversion&quot;)
+         orig_images, repro_images, conversion_info = converter.convert_document_pair(
+             args.original, args.reproduction
+         )
+         
+         if not orig_images or not repro_images:
+             print(&quot;Error: Failed to convert PDFs to images&quot;)
+             sys.exit(1)
+         
+         # Detect orientation
+         detector = OrientationDetector(output_dir=args.output)
+         report = detector.analyze_document_orientation(orig_images, repro_images)
+         
+         # Print summary
+         overall = report[&#39;overall_analysis&#39;]
+         print(f&quot;\n{&#39;=&#39;*70}&quot;)
+         print(&quot;ORIENTATION ANALYSIS SUMMARY&quot;)
+         print(f&quot;{&#39;=&#39;*70}&quot;)
+         print(f&quot;Pages analyzed: {report[&#39;total_pages&#39;]}&quot;)
+         print(f&quot;Most common rotation: {overall[&#39;most_common_coarse_rotation&#39;]}°&quot;)
+         print(f&quot;Rotation consistency: {overall[&#39;rotation_consistency&#39;]:.1%}&quot;)
+         print(f&quot;Average confidence: {overall[&#39;average_confidence&#39;]:.3f}&quot;)
+         print(f&quot;Average fine adjustment: {overall[&#39;average_fine_adjustment&#39;]:.1f}°&quot;)
+         
+         if overall[&#39;recommendations&#39;]:
+             print(f&quot;\nRecommendations:&quot;)
+             for rec in overall[&#39;recommendations&#39;]:
+                 print(f&quot;• {rec}&quot;)
+         
+         print(f&quot;\nResults saved to: {args.output}&quot;)
+         
+     except Exception as e:
+         print(f&quot;Error during orientation detection: {e}&quot;)
+         sys.exit(1)
+ 
+ 
+ if __name__ == &quot;__main__&quot;:
+     main()
```

</details>

</tool-use>

---

Now let's create a test script for the orientation detection:

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **code_edit** • Edit file: scanner/AOI/orientation/test_orientation.py</summary>

**Chunk 1**
Lines added: 224, lines removed: 1

```diff
@@ -1,1 +1,224 @@
- 
+ #!/usr/bin/env python3
+ &quot;&quot;&quot;
+ Test script for orientation detection on microfilm samples.
+ 
+ Tests orientation detection capabilities on the available test cases.
+ &quot;&quot;&quot;
+ 
+ import os
+ import sys
+ from pathlib import Path
+ from pdf_converter import PDFConverter
+ from orientation_detector import OrientationDetector
+ 
+ 
+ def main():
+     &quot;&quot;&quot;Run orientation detection tests on all available test cases.&quot;&quot;&quot;
+     
+     # Test file paths (relative to scanner root)
+     test_dir = Path(&quot;../../test&quot;)
+     original_pdf = test_dir / &quot;orig&quot; / &quot;1427004500479387.pdf&quot;
+     
+     test_cases = {
+         &quot;OK&quot;: test_dir / &quot;scan&quot; / &quot;1427004500479387_SCAN_OK.pdf&quot;,
+         &quot;NG&quot;: test_dir / &quot;scan&quot; / &quot;1427004500479387_SCAN_NG.pdf&quot;, 
+         &quot;BK&quot;: test_dir / &quot;scan&quot; / &quot;1427004500479387_SCAN_BK.pdf&quot;
+     }
+     
+     # Verify files exist
+     if not original_pdf.exists():
+         print(f&quot;Error: Original file not found: {original_pdf}&quot;)
+         sys.exit(1)
+     
+     available_cases = {}
+     for case_name, case_path in test_cases.items():
+         if case_path.exists():
+             available_cases[case_name] = case_path
+         else:
+             print(f&quot;Warning: Test case {case_name} not found: {case_path}&quot;)
+     
+     if not available_cases:
+         print(&quot;Error: No test cases found!&quot;)
+         sys.exit(1)
+     
+     # Run orientation detection for each test case
+     results = {}
+     
+     for case_name, case_path in available_cases.items():
+         print(f&quot;\n{&#39;=&#39;*80}&quot;)
+         print(f&quot;TESTING ORIENTATION DETECTION: {case_name}&quot;)
+         print(f&quot;{&#39;=&#39;*80}&quot;)
+         
+         try:
+             # Create case-specific output directories
+             conversion_dir = f&quot;orientation_test_{case_name.lower()}/conversion&quot;
+             analysis_dir = f&quot;orientation_test_{case_name.lower()}/analysis&quot;
+             
+             # Step 1: Convert PDFs to images
+             print(&quot;Step 1: Converting PDFs to images...&quot;)
+             converter = PDFConverter(dpi=300, output_dir=conversion_dir)
+             orig_images, repro_images, conversion_info = converter.convert_document_pair(
+                 str(original_pdf), str(case_path)
+             )
+             
+             if not orig_images or not repro_images:
+                 print(f&quot;Error: Failed to convert PDFs for case {case_name}&quot;)
+                 continue
+             
+             print(f&quot;Converted {len(orig_images)} original and {len(repro_images)} reproduction pages&quot;)
+             
+             # Step 2: Detect orientation
+             print(&quot;Step 2: Detecting orientation...&quot;)
+             detector = OrientationDetector(output_dir=analysis_dir)
+             orientation_report = detector.analyze_document_orientation(orig_images, repro_images)
+             
+             # Store results
+             results[case_name] = {
+                 &#39;success&#39;: True,
+                 &#39;conversion_info&#39;: conversion_info,
+                 &#39;orientation_report&#39;: orientation_report,
+                 &#39;output_directories&#39;: {
+                     &#39;conversion&#39;: conversion_dir,
+                     &#39;analysis&#39;: analysis_dir
+                 }
+             }
+             
+             # Print case summary
+             overall = orientation_report[&#39;overall_analysis&#39;]
+             print(f&quot;\n{case_name} Case Summary:&quot;)
+             print(f&quot;  Pages analyzed: {orientation_report[&#39;total_pages&#39;]}&quot;)
+             print(f&quot;  Most common rotation: {overall[&#39;most_common_coarse_rotation&#39;]}°&quot;)
+             print(f&quot;  Average confidence: {overall[&#39;average_confidence&#39;]:.3f}&quot;)
+             print(f&quot;  Average fine adjustment: {overall[&#39;average_fine_adjustment&#39;]:.1f}°&quot;)
+             
+             if overall[&#39;recommendations&#39;]:
+                 print(f&quot;  Recommendations:&quot;)
+                 for rec in overall[&#39;recommendations&#39;]:
+                     print(f&quot;    • {rec}&quot;)
+             
+         except Exception as e:
+             print(f&quot;Error testing case {case_name}: {e}&quot;)
+             results[case_name] = {
+                 &#39;success&#39;: False,
+                 &#39;error&#39;: str(e)
+             }
+             continue
+     
+     # Print comparative analysis
+     print_comparative_analysis(results)
+ 
+ 
+ def print_comparative_analysis(results: dict):
+     &quot;&quot;&quot;Print comparative analysis of all test cases.&quot;&quot;&quot;
+     
+     print(f&quot;\n{&#39;=&#39;*100}&quot;)
+     print(&quot;COMPARATIVE ORIENTATION ANALYSIS&quot;)
+     print(f&quot;{&#39;=&#39;*100}&quot;)
+     
+     # Table header
+     print(f&quot;{&#39;Case&#39;:&lt;8} {&#39;Success&#39;:&lt;8} {&#39;Pages&#39;:&lt;6} {&#39;Common Rot&#39;:&lt;11} {&#39;Avg Conf&#39;:&lt;9} {&#39;Fine Adj&#39;:&lt;9} {&#39;Quality&#39;:&lt;15}&quot;)
+     print(f&quot;{&#39;-&#39;*8} {&#39;-&#39;*8} {&#39;-&#39;*6} {&#39;-&#39;*11} {&#39;-&#39;*9} {&#39;-&#39;*9} {&#39;-&#39;*15}&quot;)
+     
+     successful_cases = []
+     
+     for case_name, result in results.items():
+         if result[&#39;success&#39;]:
+             report = result[&#39;orientation_report&#39;]
+             overall = report[&#39;overall_analysis&#39;]
+             
+             pages = report[&#39;total_pages&#39;]
+             common_rot = f&quot;{overall[&#39;most_common_coarse_rotation&#39;]}°&quot;
+             avg_conf = f&quot;{overall[&#39;average_confidence&#39;]:.3f}&quot;
+             fine_adj = f&quot;{overall[&#39;average_fine_adjustment&#39;]:.1f}°&quot;
+             
+             # Quality summary
+             quality_dist = overall[&#39;quality_distribution&#39;]
+             quality_summary = f&quot;H:{quality_dist[&#39;HIGH&#39;]} M:{quality_dist[&#39;MEDIUM&#39;]} L:{quality_dist[&#39;LOW&#39;]}&quot;
+             
+             print(f&quot;{case_name:&lt;8} {&#39;✓&#39;:&lt;8} {pages:&lt;6} {common_rot:&lt;11} {avg_conf:&lt;9} {fine_adj:&lt;9} {quality_summary:&lt;15}&quot;)
+             successful_cases.append((case_name, overall))
+         else:
+             print(f&quot;{case_name:&lt;8} {&#39;✗&#39;:&lt;8} {&#39;N/A&#39;:&lt;6} {&#39;N/A&#39;:&lt;11} {&#39;N/A&#39;:&lt;9} {&#39;N/A&#39;:&lt;9} {&#39;Failed&#39;:&lt;15}&quot;)
+     
+     # Analysis insights
+     if len(successful_cases) &gt; 1:
+         print(f&quot;\n{&#39;=&#39;*100}&quot;)
+         print(&quot;ANALYSIS INSIGHTS&quot;)
+         print(f&quot;{&#39;=&#39;*100}&quot;)
+         
+         # Compare rotation patterns
+         rotations = [overall[&#39;most_common_coarse_rotation&#39;] for _, overall in successful_cases]
+         confidences = [overall[&#39;average_confidence&#39;] for _, overall in successful_cases]
+         fine_adjustments = [overall[&#39;average_fine_adjustment&#39;] for _, overall in successful_cases]
+         
+         print(f&quot;\nRotation Pattern Analysis:&quot;)
+         rotation_set = set(rotations)
+         if len(rotation_set) == 1:
+             print(f&quot;  ✓ Consistent rotation across all cases: {rotations[0]}°&quot;)
+         else:
+             print(f&quot;  ⚠ Inconsistent rotations detected:&quot;)
+             for case_name, overall in successful_cases:
+                 print(f&quot;    {case_name}: {overall[&#39;most_common_coarse_rotation&#39;]}°&quot;)
+         
+         print(f&quot;\nConfidence Analysis:&quot;)
+         avg_confidence = sum(confidences) / len(confidences)
+         print(f&quot;  Average confidence across cases: {avg_confidence:.3f}&quot;)
+         if avg_confidence &gt; 0.8:
+             print(f&quot;  ✓ High confidence in orientation detection&quot;)
+         elif avg_confidence &gt; 0.6:
+             print(f&quot;  ⚠ Moderate confidence - some uncertainty in detection&quot;)
+         else:
+             print(f&quot;  ✗ Low confidence - manual verification recommended&quot;)
+         
+         print(f&quot;\nFine Adjustment Analysis:&quot;)
+         avg_fine_adj = sum(fine_adjustments) / len(fine_adjustments)
+         print(f&quot;  Average fine adjustment: {avg_fine_adj:.1f}°&quot;)
+         if abs(avg_fine_adj) &lt; 1:
+             print(f&quot;  ✓ Minimal systematic skew&quot;)
+         elif abs(avg_fine_adj) &lt; 3:
+             print(f&quot;  ⚠ Small systematic skew detected&quot;)
+         else:
+             print(f&quot;  ✗ Significant systematic skew - check scanning alignment&quot;)
+         
+         # Case-specific insights
+         print(f&quot;\nCase-Specific Insights:&quot;)
+         for case_name, overall in successful_cases:
+             insights = []
+             
+             if overall[&#39;rotation_consistency&#39;] &lt; 0.8:
+                 insights.append(&quot;inconsistent page rotations&quot;)
+             if overall[&#39;average_confidence&#39;] &lt; 0.6:
+                 insights.append(&quot;low detection confidence&quot;)
+             if abs(overall[&#39;average_fine_adjustment&#39;]) &gt; 3:
+                 insights.append(f&quot;systematic skew ({overall[&#39;average_fine_adjustment&#39;]:.1f}°)&quot;)
+             if overall[&#39;pages_with_warnings&#39;] &gt; 0:
+                 insights.append(f&quot;{overall[&#39;pages_with_warnings&#39;]} pages with warnings&quot;)
+             
+             if insights:
+                 print(f&quot;  {case_name}: {&#39;, &#39;.join(insights)}&quot;)
+             else:
+                 print(f&quot;  {case_name}: no significant issues detected&quot;)
+     
+     print(f&quot;\n{&#39;=&#39;*100}&quot;)
+     print(&quot;RECOMMENDATIONS&quot;)
+     print(f&quot;{&#39;=&#39;*100}&quot;)
+     
+     # Generate overall recommendations
+     if successful_cases:
+         all_recommendations = set()
+         for _, overall in successful_cases:
+             all_recommendations.update(overall.get(&#39;recommendations&#39;, []))
+         
+         if all_recommendations:
+             for rec in all_recommendations:
+                 print(f&quot;• {rec}&quot;)
+         else:
+             print(&quot;• No specific recommendations - orientation detection appears reliable&quot;)
+     else:
+         print(&quot;• No successful cases to analyze&quot;)
+     
+     print(f&quot;\nDetailed results saved in respective orientation_test_* directories&quot;)
+ 
+ 
+ if __name__ == &quot;__main__&quot;:
+     main()
```

</details>

</tool-use>

---

<tool-use data-tool-type="generic" data-tool-name="todo_write">
<details><summary>Tool use: **todo_write** • Todo List</summary>

- [x] Analyze current system architecture and identify improvement areas
- [ ] Design comprehensive AOI framework with configurable inspection modules
- [ ] Plan GUI interface with preview windows and real-time visualization
- [ ] Design advanced defect detection algorithms (positioning, content, quality)
- [ ] Plan flexible configuration system for thresholds and inspection parameters
- [x] Design image processing pipeline for microfilm defect detection
- [ ] Implement overexposure/underexposure detection algorithms
- [ ] Implement scratch and artifact detection
- [ ] Implement OCR-based readability assessment
- [x] Create brightness/contrast analysis script for test PDFs
- [x] Create orientation detection module for PDF alignment

</details>
</tool-use>

---

Perfect! I've created a comprehensive orientation detection system. Here's what we now have:

## **Orientation Detection Module Structure:**

```
scanner/AOI/orientation/
├── pdf_converter.py          # Robust PDF→Image conversion
├── orientation_detector.py   # Image-level orientation detection  
└── test_orientation.py       # Test script for all cases
```

## **Key Features of the Orientation System:**

### **1. Robust PDF Conversion (`pdf_converter.py`)**
- **Dual conversion methods**: pdf2image and PyMuPDF for reliability
- **Quality metrics**: Analyzes conversion quality and consistency
- **Size normalization**: Handles dimension mismatches
- **Intermediate saving**: Saves converted images for debugging
- **Comprehensive reporting**: JSON reports with all conversion details

### **2. Advanced Orientation Detection (`orientation_detector.py`)**

#### **Two-Stage Detection Process:**
1. **Coarse Detection**: Tests 0°, 90°, 180°, 270° rotations
2. **Fine Detection**: ±10° fine adjustment in 0.5° increments

#### **Smart Image Processing:**
- **Edge-based matching**: Uses Canny edge detection for robust orientation detection
- **Speed optimization**: Resizes images for faster processing
- **Multiple scoring methods**: Combines SSIM and cross-correlation
- **Quality assessment**: Evaluates detection confidence and reliability

#### **Comprehensive Analysis:**
- **Per-page detection**: Handles mixed orientations within documents
- **Pattern analysis**: Identifies systematic rotation issues
- **Confidence scoring**: Quantifies detection reliability
- **Warning system**: Flags potential issues

### **3. Visual Analysis & Reporting:**
- **Before/after visualization**: Shows original, reproduction, and corrected images
- **Score plots**: Displays rotation scores and fine adjustment curves
- **Quality indicators**: Color-coded confidence levels
- **Detailed reports**: JSON format with all detection data

## **Why Image-Level Detection is Superior:**

1. **Flexibility**: Can handle any initial orientation of either PDF
2. **Accuracy**: Direct image analysis without PDF metadata dependencies  
3. **Robustness**: Works regardless of how PDFs were created/scanned
4. **Granular Control**: Per-page detection handles mixed orientations
5. **Quality Feedback**: Provides confidence metrics for each detection

## **How to Test:**

```bash
cd scanner/AOI/orientation
python test_orientation.py
```

This will:
1. Convert all your test PDFs (OK, NG, BK) to images
2. Detect orientation for each case
3. Generate detailed visualizations
4. Provide comparative analysis across all cases
5. Give specific recommendations for each case

The system will detect:
- **Systematic rotation** (e.g., all pages rotated 90°)
- **Scanning skew** (small angle corrections needed)
- **Inconsistent orientations** (mixed rotations within document)
- **Detection confidence** (how reliable the orientation detection is)

This gives you a solid foundation for the AOI system - now any subsequent analysis (brightness, scratches, etc.) can work with properly oriented images!

---

_**User (2025-11-08 20:38Z)**_

@powershell (19-20) 

---

_**Agent (model claude-4-sonnet, mode Agent)**_

I see the issue - the path creation is using forward slashes but Windows needs backslashes, and the directories aren't being created properly. Let me fix the path handling in the test script:

---

<tool-use data-tool-type="write" data-tool-name="search_replace">

<details><summary>Tool use: **code_edit** • Edit file: scanner/AOI/orientation/test_orientation.py</summary>

**Chunk 1**
Lines added: 8, lines removed: 3

```diff
@@ -52,10 +52,15 @@
          try:
              # Create case-specific output directories
-             conversion_dir = f&quot;orientation_test_{case_name.lower()}/conversion&quot;
-             analysis_dir = f&quot;orientation_test_{case_name.lower()}/analysis&quot;
+             base_dir = Path(f&quot;orientation_test_{case_name.lower()}&quot;)
+             conversion_dir = base_dir / &quot;conversion&quot;
+             analysis_dir = base_dir / &quot;analysis&quot;
+             
+             # Ensure directories exist
+             conversion_dir.mkdir(parents=True, exist_ok=True)
+             analysis_dir.mkdir(parents=True, exist_ok=True)
              
              # Step 1: Convert PDFs to images
              print(&quot;Step 1: Converting PDFs to images...&quot;)
-             converter = PDFConverter(dpi=300, output_dir=conversion_dir)
+             converter = PDFConverter(dpi=300, output_dir=str(conversion_dir))
              orig_images, repro_images, conversion_info = converter.convert_document_pair(
                  str(original_pdf), str(case_path)
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="search_replace">

<details><summary>Tool use: **code_edit** • Edit file: scanner/AOI/orientation/test_orientation.py</summary>

**Chunk 1**
Lines added: 1, lines removed: 1

```diff
@@ -75,5 +75,5 @@
              # Step 2: Detect orientation
              print(&quot;Step 2: Detecting orientation...&quot;)
-             detector = OrientationDetector(output_dir=analysis_dir)
+             detector = OrientationDetector(output_dir=str(analysis_dir))
              orientation_report = detector.analyze_document_orientation(orig_images, repro_images)
              
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="search_replace">

<details><summary>Tool use: **code_edit** • Edit file: scanner/AOI/orientation/test_orientation.py</summary>

**Chunk 1**
Lines added: 2, lines removed: 2

```diff
@@ -84,6 +84,6 @@
                  &#39;orientation_report&#39;: orientation_report,
                  &#39;output_directories&#39;: {
-                     &#39;conversion&#39;: conversion_dir,
-                     &#39;analysis&#39;: analysis_dir
+                     &#39;conversion&#39;: str(conversion_dir),
+                     &#39;analysis&#39;: str(analysis_dir)
                  }
              }
```

</details>

</tool-use>

---

The issue was that the script was trying to create directories using string paths instead of properly creating them first. Now the script:

1. **Creates Path objects** for proper cross-platform path handling
2. **Explicitly creates directories** using `mkdir(parents=True, exist_ok=True)`
3. **Converts to strings** when passing to the classes that expect string paths

This should fix the Windows path issue. The script will now properly create the directory structure:

```
orientation_test_ng/
├── conversion/
└── analysis/

orientation_test_ok/
├── conversion/
└── analysis/

orientation_test_bk/
├── conversion/
└── analysis/
```

Try running the test again:

```bash
cd scanner/AOI/orientation
python test_orientation.py
```

The path handling should now work correctly on Windows!

---

_**User (2025-11-08 20:38Z)**_

but this doesnt align the orientations?

---

_**Agent (model claude-4-sonnet, mode Agent)**_

You're absolutely right! The orientation detector only **detects** the rotation needed but doesn't actually **apply** the correction to align the images. Let me create an alignment module that actually corrects the orientation and provides aligned images for further analysis.

---

<tool-use data-tool-type="generic" data-tool-name="todo_write">
<details><summary>Tool use: **todo_write** • Todo List</summary>

- [x] Analyze current system architecture and identify improvement areas
- [ ] Design comprehensive AOI framework with configurable inspection modules
- [ ] Plan GUI interface with preview windows and real-time visualization
- [ ] Design advanced defect detection algorithms (positioning, content, quality)
- [ ] Plan flexible configuration system for thresholds and inspection parameters
- [x] Design image processing pipeline for microfilm defect detection
- [ ] Implement overexposure/underexposure detection algorithms
- [ ] Implement scratch and artifact detection
- [ ] Implement OCR-based readability assessment
- [x] Create brightness/contrast analysis script for test PDFs
- [x] Create orientation detection module for PDF alignment
- [-] Create image alignment module that applies orientation corrections

</details>
</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **code_edit** • Edit file: scanner/AOI/orientation/image_aligner.py</summary>

**Chunk 1**
Lines added: 528, lines removed: 1

```diff
@@ -1,1 +1,528 @@
- 
+ #!/usr/bin/env python3
+ &quot;&quot;&quot;
+ Image Alignment Module
+ 
+ Applies orientation corrections to align reproduction images with originals.
+ Provides aligned image pairs for subsequent AOI analysis.
+ &quot;&quot;&quot;
+ 
+ import os
+ import sys
+ from pathlib import Path
+ from typing import List, Tuple, Dict, Any, Optional
+ import numpy as np
+ import cv2
+ import matplotlib.pyplot as plt
+ import json
+ from datetime import datetime
+ from pdf_converter import PDFConverter
+ from orientation_detector import OrientationDetector
+ 
+ 
+ class ImageAligner:
+     &quot;&quot;&quot;
+     Applies orientation corrections to align images for AOI analysis.
+     &quot;&quot;&quot;
+     
+     def __init__(self, output_dir: str = &quot;aligned_images&quot;):
+         self.output_dir = Path(output_dir)
+         self.output_dir.mkdir(exist_ok=True)
+         
+         # Alignment parameters
+         self.save_aligned_images = True  # Save aligned images to disk
+         self.create_comparison_plots = True  # Create before/after comparison plots
+         
+         # Size normalization options
+         self.normalize_sizes = True
+         self.target_size_method = &quot;original&quot;  # &quot;original&quot;, &quot;reproduction&quot;, &quot;largest&quot;, &quot;custom&quot;
+         self.custom_size = None  # (width, height) if using custom
+         
+     def rotate_image(self, img: np.ndarray, angle: float, 
+                     maintain_size: bool = False) -&gt; np.ndarray:
+         &quot;&quot;&quot;
+         Rotate image by specified angle.
+         
+         Args:
+             img: Input image
+             angle: Rotation angle in degrees (positive = counterclockwise)
+             maintain_size: If True, keeps original image dimensions (may crop)
+                           If False, expands canvas to fit rotated image
+             
+         Returns:
+             Rotated image
+         &quot;&quot;&quot;
+         if abs(angle) &lt; 0.01:  # No rotation needed
+             return img.copy()
+         
+         h, w = img.shape[:2]
+         
+         # Handle 90-degree increments efficiently
+         if abs(angle - 90) &lt; 0.01:
+             return cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)
+         elif abs(angle - 180) &lt; 0.01:
+             return cv2.rotate(img, cv2.ROTATE_180)
+         elif abs(angle - 270) &lt; 0.01 or abs(angle + 90) &lt; 0.01:
+             return cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)
+         
+         # Fine rotation using affine transformation
+         center = (w // 2, h // 2)
+         rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)
+         
+         if maintain_size:
+             # Keep original dimensions (may crop content)
+             rotated = cv2.warpAffine(img, rotation_matrix, (w, h))
+         else:
+             # Expand canvas to fit rotated image (no cropping)
+             cos_angle = abs(rotation_matrix[0, 0])
+             sin_angle = abs(rotation_matrix[0, 1])
+             new_w = int((h * sin_angle) + (w * cos_angle))
+             new_h = int((h * cos_angle) + (w * sin_angle))
+             
+             # Adjust rotation matrix for new dimensions
+             rotation_matrix[0, 2] += (new_w / 2) - center[0]
+             rotation_matrix[1, 2] += (new_h / 2) - center[1]
+             
+             rotated = cv2.warpAffine(img, rotation_matrix, (new_w, new_h))
+         
+         return rotated
+     
+     def normalize_image_sizes(self, img1: np.ndarray, img2: np.ndarray, 
+                             method: str = &quot;original&quot;) -&gt; Tuple[np.ndarray, np.ndarray]:
+         &quot;&quot;&quot;
+         Normalize sizes of two images.
+         
+         Args:
+             img1: First image (typically original)
+             img2: Second image (typically reproduction)
+             method: Size normalization method
+                    - &quot;original&quot;: Use img1 size
+                    - &quot;reproduction&quot;: Use img2 size  
+                    - &quot;largest&quot;: Use largest dimensions
+                    - &quot;custom&quot;: Use self.custom_size
+                    
+         Returns:
+             Tuple of size-normalized images
+         &quot;&quot;&quot;
+         h1, w1 = img1.shape[:2]
+         h2, w2 = img2.shape[:2]
+         
+         if method == &quot;original&quot;:
+             target_size = (w1, h1)
+         elif method == &quot;reproduction&quot;:
+             target_size = (w2, h2)
+         elif method == &quot;largest&quot;:
+             target_size = (max(w1, w2), max(h1, h2))
+         elif method == &quot;custom&quot; and self.custom_size:
+             target_size = self.custom_size
+         else:
+             # Default to original size
+             target_size = (w1, h1)
+         
+         # Resize images if needed
+         if (h1, w1) != (target_size[1], target_size[0]):
+             img1_resized = cv2.resize(img1, target_size)
+         else:
+             img1_resized = img1.copy()
+             
+         if (h2, w2) != (target_size[1], target_size[0]):
+             img2_resized = cv2.resize(img2, target_size)
+         else:
+             img2_resized = img2.copy()
+         
+         return img1_resized, img2_resized
+     
+     def align_single_page(self, original_img: np.ndarray, reproduction_img: np.ndarray,
+                          rotation_angle: float, page_num: int) -&gt; Dict[str, Any]:
+         &quot;&quot;&quot;
+         Align a single page pair.
+         
+         Args:
+             original_img: Original page image
+             reproduction_img: Reproduction page image
+             rotation_angle: Rotation angle to apply to reproduction
+             page_num: Page number for naming/tracking
+             
+         Returns:
+             Dictionary with alignment results
+         &quot;&quot;&quot;
+         print(f&quot;Aligning page {page_num} (rotation: {rotation_angle:.1f}°)...&quot;)
+         
+         # Step 1: Apply rotation correction
+         aligned_reproduction = self.rotate_image(reproduction_img, rotation_angle, 
+                                                maintain_size=False)
+         
+         # Step 2: Normalize sizes if requested
+         if self.normalize_sizes:
+             original_normalized, aligned_normalized = self.normalize_image_sizes(
+                 original_img, aligned_reproduction, method=self.target_size_method
+             )
+         else:
+             original_normalized = original_img.copy()
+             aligned_normalized = aligned_reproduction.copy()
+         
+         # Step 3: Calculate alignment quality metrics
+         alignment_metrics = self._calculate_alignment_metrics(original_normalized, 
+                                                             aligned_normalized)
+         
+         # Step 4: Save aligned images if requested
+         saved_paths = {}
+         if self.save_aligned_images:
+             saved_paths = self._save_aligned_images(original_normalized, aligned_normalized,
+                                                   reproduction_img, page_num, rotation_angle)
+         
+         # Step 5: Create comparison visualization if requested
+         comparison_path = None
+         if self.create_comparison_plots:
+             comparison_path = self._create_alignment_comparison(
+                 original_img, reproduction_img, original_normalized, aligned_normalized,
+                 page_num, rotation_angle, alignment_metrics
+             )
+         
+         result = {
+             &#39;page&#39;: page_num,
+             &#39;rotation_applied&#39;: rotation_angle,
+             &#39;original_shape&#39;: original_img.shape,
+             &#39;reproduction_original_shape&#39;: reproduction_img.shape,
+             &#39;aligned_shape&#39;: aligned_normalized.shape,
+             &#39;size_normalized&#39;: self.normalize_sizes,
+             &#39;alignment_metrics&#39;: alignment_metrics,
+             &#39;saved_paths&#39;: saved_paths,
+             &#39;comparison_visualization&#39;: comparison_path,
+             &#39;aligned_images&#39;: {
+                 &#39;original&#39;: original_normalized,
+                 &#39;reproduction&#39;: aligned_normalized
+             }
+         }
+         
+         return result
+     
+     def _calculate_alignment_metrics(self, img1: np.ndarray, img2: np.ndarray) -&gt; Dict[str, float]:
+         &quot;&quot;&quot;Calculate metrics to assess alignment quality.&quot;&quot;&quot;
+         
+         from skimage.metrics import structural_similarity as ssim
+         
+         metrics = {}
+         
+         try:
+             # SSIM score
+             ssim_score, _ = ssim(img1, img2, full=True)
+             metrics[&#39;ssim&#39;] = float(ssim_score)
+         except Exception:
+             metrics[&#39;ssim&#39;] = 0.0
+         
+         try:
+             # Normalized cross-correlation
+             img1_norm = cv2.normalize(img1.astype(np.float32), None, 0, 1, cv2.NORM_MINMAX)
+             img2_norm = cv2.normalize(img2.astype(np.float32), None, 0, 1, cv2.NORM_MINMAX)
+             correlation = cv2.matchTemplate(img1_norm, img2_norm, cv2.TM_CCOEFF_NORMED)[0, 0]
+             metrics[&#39;correlation&#39;] = float(correlation)
+         except Exception:
+             metrics[&#39;correlation&#39;] = 0.0
+         
+         try:
+             # Mean squared error (lower is better)
+             mse = np.mean((img1.astype(np.float32) - img2.astype(np.float32)) ** 2)
+             metrics[&#39;mse&#39;] = float(mse)
+         except Exception:
+             metrics[&#39;mse&#39;] = float(&#39;inf&#39;)
+         
+         try:
+             # Peak signal-to-noise ratio
+             if metrics[&#39;mse&#39;] &gt; 0:
+                 psnr = 20 * np.log10(255.0 / np.sqrt(metrics[&#39;mse&#39;]))
+                 metrics[&#39;psnr&#39;] = float(psnr)
+             else:
+                 metrics[&#39;psnr&#39;] = float(&#39;inf&#39;)
+         except Exception:
+             metrics[&#39;psnr&#39;] = 0.0
+         
+         return metrics
+     
+     def _save_aligned_images(self, original: np.ndarray, aligned_repro: np.ndarray,
+                            original_repro: np.ndarray, page_num: int, 
+                            rotation: float) -&gt; Dict[str, str]:
+         &quot;&quot;&quot;Save aligned images to disk.&quot;&quot;&quot;
+         
+         saved_paths = {}
+         
+         # Create page-specific directory
+         page_dir = self.output_dir / f&quot;page_{page_num:03d}&quot;
+         page_dir.mkdir(exist_ok=True)
+         
+         # Save original (normalized)
+         orig_path = page_dir / f&quot;original_normalized.png&quot;
+         cv2.imwrite(str(orig_path), original)
+         saved_paths[&#39;original_normalized&#39;] = str(orig_path)
+         
+         # Save aligned reproduction
+         aligned_path = page_dir / f&quot;reproduction_aligned_{rotation:.1f}deg.png&quot;
+         cv2.imwrite(str(aligned_path), aligned_repro)
+         saved_paths[&#39;reproduction_aligned&#39;] = str(aligned_path)
+         
+         # Save original reproduction (before alignment)
+         orig_repro_path = page_dir / f&quot;reproduction_original.png&quot;
+         cv2.imwrite(str(orig_repro_path), original_repro)
+         saved_paths[&#39;reproduction_original&#39;] = str(orig_repro_path)
+         
+         return saved_paths
+     
+     def _create_alignment_comparison(self, original_img: np.ndarray, reproduction_img: np.ndarray,
+                                    aligned_original: np.ndarray, aligned_reproduction: np.ndarray,
+                                    page_num: int, rotation: float, 
+                                    metrics: Dict[str, float]) -&gt; str:
+         &quot;&quot;&quot;Create before/after alignment comparison visualization.&quot;&quot;&quot;
+         
+         fig, axes = plt.subplots(2, 3, figsize=(18, 12))
+         fig.suptitle(f&#39;Page {page_num} Alignment Results (Rotation: {rotation:.1f}°)&#39;, 
+                      fontsize=16, fontweight=&#39;bold&#39;)
+         
+         # Before alignment
+         axes[0, 0].imshow(original_img, cmap=&#39;gray&#39;)
+         axes[0, 0].set_title(&#39;Original&#39;)
+         axes[0, 0].axis(&#39;off&#39;)
+         
+         axes[0, 1].imshow(reproduction_img, cmap=&#39;gray&#39;)
+         axes[0, 1].set_title(&#39;Reproduction (Before)&#39;)
+         axes[0, 1].axis(&#39;off&#39;)
+         
+         # Difference before alignment (if same size)
+         if original_img.shape == reproduction_img.shape:
+             diff_before = cv2.absdiff(original_img, reproduction_img)
+             axes[0, 2].imshow(diff_before, cmap=&#39;hot&#39;)
+             axes[0, 2].set_title(&#39;Difference (Before)&#39;)
+         else:
+             axes[0, 2].text(0.5, 0.5, &#39;Size Mismatch\nCannot Show Difference&#39;, 
+                            ha=&#39;center&#39;, va=&#39;center&#39;, transform=axes[0, 2].transAxes)
+             axes[0, 2].set_title(&#39;Difference (Before)&#39;)
+         axes[0, 2].axis(&#39;off&#39;)
+         
+         # After alignment
+         axes[1, 0].imshow(aligned_original, cmap=&#39;gray&#39;)
+         axes[1, 0].set_title(&#39;Original (Normalized)&#39;)
+         axes[1, 0].axis(&#39;off&#39;)
+         
+         axes[1, 1].imshow(aligned_reproduction, cmap=&#39;gray&#39;)
+         axes[1, 1].set_title(&#39;Reproduction (Aligned)&#39;)
+         axes[1, 1].axis(&#39;off&#39;)
+         
+         # Difference after alignment
+         diff_after = cv2.absdiff(aligned_original, aligned_reproduction)
+         axes[1, 2].imshow(diff_after, cmap=&#39;hot&#39;)
+         axes[1, 2].set_title(&#39;Difference (After)&#39;)
+         axes[1, 2].axis(&#39;off&#39;)
+         
+         # Add metrics text
+         metrics_text = f&quot;&quot;&quot;
+ Alignment Quality Metrics:
+ 
+ SSIM: {metrics.get(&#39;ssim&#39;, 0):.4f}
+ Correlation: {metrics.get(&#39;correlation&#39;, 0):.4f}
+ MSE: {metrics.get(&#39;mse&#39;, 0):.2f}
+ PSNR: {metrics.get(&#39;psnr&#39;, 0):.2f} dB
+ 
+ Rotation Applied: {rotation:.1f}°
+ Size Normalization: {&#39;Yes&#39; if self.normalize_sizes else &#39;No&#39;}
+ Target Size Method: {self.target_size_method}
+         &quot;&quot;&quot;
+         
+         # Add text box with metrics
+         fig.text(0.02, 0.02, metrics_text, fontsize=10, fontfamily=&#39;monospace&#39;,
+                 bbox=dict(boxstyle=&quot;round,pad=0.5&quot;, facecolor=&quot;lightgray&quot;, alpha=0.8))
+         
+         plt.tight_layout()
+         
+         # Save comparison
+         comparison_path = self.output_dir / f&quot;page_{page_num:03d}_alignment_comparison.png&quot;
+         plt.savefig(comparison_path, dpi=150, bbox_inches=&#39;tight&#39;)
+         plt.close()
+         
+         return str(comparison_path)
+     
+     def align_document_pair(self, original_images: List[np.ndarray], 
+                           reproduction_images: List[np.ndarray],
+                           orientation_results: Dict[str, Any]) -&gt; Dict[str, Any]:
+         &quot;&quot;&quot;
+         Align entire document pair using orientation detection results.
+         
+         Args:
+             original_images: List of original page images
+             reproduction_images: List of reproduction page images
+             orientation_results: Results from orientation detection
+             
+         Returns:
+             Complete alignment results with aligned image pairs
+         &quot;&quot;&quot;
+         print(f&quot;\n{&#39;=&#39;*70}&quot;)
+         print(&quot;DOCUMENT ALIGNMENT&quot;)
+         print(f&quot;{&#39;=&#39;*70}&quot;)
+         
+         min_pages = min(len(original_images), len(reproduction_images))
+         page_results = []
+         aligned_pairs = []
+         
+         # Extract rotation angles from orientation results
+         page_orientations = {}
+         if &#39;page_results&#39; in orientation_results:
+             for page_result in orientation_results[&#39;page_results&#39;]:
+                 page_num = page_result[&#39;page&#39;]
+                 rotation = page_result[&#39;final_rotation&#39;]
+                 page_orientations[page_num] = rotation
+         
+         for i in range(min_pages):
+             page_num = i + 1
+             
+             # Get rotation angle for this page
+             rotation_angle = page_orientations.get(page_num, 0.0)
+             
+             # Align this page
+             alignment_result = self.align_single_page(
+                 original_images[i], reproduction_images[i], rotation_angle, page_num
+             )
+             
+             page_results.append(alignment_result)
+             
+             # Store aligned image pair
+             aligned_pairs.append({
+                 &#39;page&#39;: page_num,
+                 &#39;original&#39;: alignment_result[&#39;aligned_images&#39;][&#39;original&#39;],
+                 &#39;reproduction&#39;: alignment_result[&#39;aligned_images&#39;][&#39;reproduction&#39;],
+                 &#39;rotation_applied&#39;: rotation_angle,
+                 &#39;alignment_quality&#39;: alignment_result[&#39;alignment_metrics&#39;]
+             })
+         
+         # Calculate overall alignment statistics
+         overall_stats = self._calculate_overall_alignment_stats(page_results)
+         
+         # Generate alignment report
+         alignment_report = {
+             &#39;timestamp&#39;: datetime.now().isoformat(),
+             &#39;analysis_type&#39;: &#39;image_alignment&#39;,
+             &#39;total_pages&#39;: min_pages,
+             &#39;alignment_settings&#39;: {
+                 &#39;normalize_sizes&#39;: self.normalize_sizes,
+                 &#39;target_size_method&#39;: self.target_size_method,
+                 &#39;save_aligned_images&#39;: self.save_aligned_images,
+                 &#39;create_comparison_plots&#39;: self.create_comparison_plots
+             },
+             &#39;page_results&#39;: page_results,
+             &#39;overall_statistics&#39;: overall_stats,
+             &#39;aligned_image_pairs&#39;: len(aligned_pairs)
+         }
+         
+         # Save alignment report
+         report_path = self.output_dir / &quot;alignment_report.json&quot;
+         
+         # Remove numpy arrays before JSON serialization
+         report_for_json = alignment_report.copy()
+         for page_result in report_for_json[&#39;page_results&#39;]:
+             if &#39;aligned_images&#39; in page_result:
+                 del page_result[&#39;aligned_images&#39;]  # Remove numpy arrays
+         
+         with open(report_path, &#39;w&#39;) as f:
+             json.dump(report_for_json, f, indent=2)
+         
+         print(f&quot;\nAlignment complete!&quot;)
+         print(f&quot;Pages aligned: {len(aligned_pairs)}&quot;)
+         print(f&quot;Average SSIM: {overall_stats[&#39;average_ssim&#39;]:.4f}&quot;)
+         print(f&quot;Average correlation: {overall_stats[&#39;average_correlation&#39;]:.4f}&quot;)
+         print(f&quot;Report saved: {report_path}&quot;)
+         
+         # Return results with aligned image pairs
+         alignment_report[&#39;aligned_image_pairs&#39;] = aligned_pairs
+         return alignment_report
+     
+     def _calculate_overall_alignment_stats(self, page_results: List[Dict]) -&gt; Dict[str, float]:
+         &quot;&quot;&quot;Calculate overall alignment statistics.&quot;&quot;&quot;
+         
+         if not page_results:
+             return {}
+         
+         ssim_scores = [p[&#39;alignment_metrics&#39;][&#39;ssim&#39;] for p in page_results]
+         correlation_scores = [p[&#39;alignment_metrics&#39;][&#39;correlation&#39;] for p in page_results]
+         mse_scores = [p[&#39;alignment_metrics&#39;][&#39;mse&#39;] for p in page_results if p[&#39;alignment_metrics&#39;][&#39;mse&#39;] != float(&#39;inf&#39;)]
+         psnr_scores = [p[&#39;alignment_metrics&#39;][&#39;psnr&#39;] for p in page_results if p[&#39;alignment_metrics&#39;][&#39;psnr&#39;] != float(&#39;inf&#39;)]
+         
+         stats = {
+             &#39;average_ssim&#39;: float(np.mean(ssim_scores)) if ssim_scores else 0.0,
+             &#39;std_ssim&#39;: float(np.std(ssim_scores)) if ssim_scores else 0.0,
+             &#39;average_correlation&#39;: float(np.mean(correlation_scores)) if correlation_scores else 0.0,
+             &#39;std_correlation&#39;: float(np.std(correlation_scores)) if correlation_scores else 0.0,
+             &#39;average_mse&#39;: float(np.mean(mse_scores)) if mse_scores else 0.0,
+             &#39;average_psnr&#39;: float(np.mean(psnr_scores)) if psnr_scores else 0.0,
+             &#39;pages_processed&#39;: len(page_results)
+         }
+         
+         return stats
+ 
+ 
+ def main():
+     &quot;&quot;&quot;Main function for testing image alignment.&quot;&quot;&quot;
+     
+     import argparse
+     
+     parser = argparse.ArgumentParser(description=&#39;Align document images using orientation detection&#39;)
+     parser.add_argument(&#39;--original&#39;, required=True, help=&#39;Path to original PDF&#39;)
+     parser.add_argument(&#39;--reproduction&#39;, required=True, help=&#39;Path to reproduction PDF&#39;)
+     parser.add_argument(&#39;--output&#39;, default=&#39;alignment_test&#39;, help=&#39;Output directory&#39;)
+     parser.add_argument(&#39;--dpi&#39;, type=int, default=300, help=&#39;DPI for PDF conversion&#39;)
+     parser.add_argument(&#39;--size-method&#39;, default=&#39;original&#39;, 
+                        choices=[&#39;original&#39;, &#39;reproduction&#39;, &#39;largest&#39;],
+                        help=&#39;Size normalization method&#39;)
+     
+     args = parser.parse_args()
+     
+     # Validate input files
+     if not os.path.exists(args.original):
+         print(f&quot;Error: Original file not found: {args.original}&quot;)
+         sys.exit(1)
+     
+     if not os.path.exists(args.reproduction):
+         print(f&quot;Error: Reproduction file not found: {args.reproduction}&quot;)
+         sys.exit(1)
+     
+     try:
+         output_dir = Path(args.output)
+         
+         # Step 1: Convert PDFs to images
+         print(&quot;Step 1: Converting PDFs to images...&quot;)
+         converter = PDFConverter(dpi=args.dpi, output_dir=str(output_dir / &quot;conversion&quot;))
+         orig_images, repro_images, conversion_info = converter.convert_document_pair(
+             args.original, args.reproduction
+         )
+         
+         if not orig_images or not repro_images:
+             print(&quot;Error: Failed to convert PDFs to images&quot;)
+             sys.exit(1)
+         
+         # Step 2: Detect orientation
+         print(&quot;Step 2: Detecting orientation...&quot;)
+         detector = OrientationDetector(output_dir=str(output_dir / &quot;orientation&quot;))
+         orientation_results = detector.analyze_document_orientation(orig_images, repro_images)
+         
+         # Step 3: Align images
+         print(&quot;Step 3: Aligning images...&quot;)
+         aligner = ImageAligner(output_dir=str(output_dir / &quot;aligned&quot;))
+         aligner.target_size_method = args.size_method
+         alignment_results = aligner.align_document_pair(orig_images, repro_images, orientation_results)
+         
+         # Print summary
+         print(f&quot;\n{&#39;=&#39;*70}&quot;)
+         print(&quot;ALIGNMENT SUMMARY&quot;)
+         print(f&quot;{&#39;=&#39;*70}&quot;)
+         print(f&quot;Pages processed: {alignment_results[&#39;total_pages&#39;]}&quot;)
+         print(f&quot;Average SSIM after alignment: {alignment_results[&#39;overall_statistics&#39;][&#39;average_ssim&#39;]:.4f}&quot;)
+         print(f&quot;Average correlation after alignment: {alignment_results[&#39;overall_statistics&#39;][&#39;average_correlation&#39;]:.4f}&quot;)
+         print(f&quot;Size normalization method: {args.size_method}&quot;)
+         print(f&quot;Results saved to: {args.output}&quot;)
+         
+         # The aligned image pairs are now available in:
+         # alignment_results[&#39;aligned_image_pairs&#39;]
+         print(f&quot;\nAligned image pairs ready for further AOI analysis!&quot;)
+         
+     except Exception as e:
+         print(f&quot;Error during alignment: {e}&quot;)
+         sys.exit(1)
+ 
+ 
+ if __name__ == &quot;__main__&quot;:
+     main()
```

</details>

</tool-use>

---

Now let's create a comprehensive test script that does the complete pipeline: detection + alignment:

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **code_edit** • Edit file: scanner/AOI/orientation/test_complete_alignment.py</summary>

**Chunk 1**
Lines added: 370, lines removed: 1

```diff
@@ -1,1 +1,370 @@
- 
+ #!/usr/bin/env python3
+ &quot;&quot;&quot;
+ Complete Alignment Test Script
+ 
+ Tests the full pipeline: PDF conversion → Orientation detection → Image alignment
+ Provides properly aligned image pairs ready for AOI analysis.
+ &quot;&quot;&quot;
+ 
+ import os
+ import sys
+ from pathlib import Path
+ from pdf_converter import PDFConverter
+ from orientation_detector import OrientationDetector
+ from image_aligner import ImageAligner
+ 
+ 
+ def test_complete_alignment_pipeline(original_pdf: str, reproduction_pdf: str, 
+                                    case_name: str, output_base: str = &quot;complete_alignment_test&quot;):
+     &quot;&quot;&quot;
+     Test complete alignment pipeline for a single case.
+     
+     Args:
+         original_pdf: Path to original PDF
+         reproduction_pdf: Path to reproduction PDF
+         case_name: Name of test case (e.g., &quot;OK&quot;, &quot;NG&quot;, &quot;BK&quot;)
+         output_base: Base output directory
+         
+     Returns:
+         Dictionary with complete results including aligned image pairs
+     &quot;&quot;&quot;
+     
+     print(f&quot;\n{&#39;=&#39;*80}&quot;)
+     print(f&quot;COMPLETE ALIGNMENT PIPELINE: {case_name}&quot;)
+     print(f&quot;{&#39;=&#39;*80}&quot;)
+     
+     # Create case-specific output directory
+     case_dir = Path(output_base) / f&quot;case_{case_name.lower()}&quot;
+     case_dir.mkdir(parents=True, exist_ok=True)
+     
+     try:
+         # Step 1: Convert PDFs to images
+         print(&quot;\n1. CONVERTING PDFs TO IMAGES&quot;)
+         print(&quot;-&quot; * 40)
+         
+         conversion_dir = case_dir / &quot;01_conversion&quot;
+         converter = PDFConverter(dpi=300, output_dir=str(conversion_dir))
+         orig_images, repro_images, conversion_info = converter.convert_document_pair(
+             original_pdf, reproduction_pdf
+         )
+         
+         if not orig_images or not repro_images:
+             raise ValueError(&quot;Failed to convert PDFs to images&quot;)
+         
+         print(f&quot;✓ Converted {len(orig_images)} original and {len(repro_images)} reproduction pages&quot;)
+         
+         # Step 2: Detect orientation
+         print(&quot;\n2. DETECTING ORIENTATION&quot;)
+         print(&quot;-&quot; * 40)
+         
+         orientation_dir = case_dir / &quot;02_orientation_detection&quot;
+         detector = OrientationDetector(output_dir=str(orientation_dir))
+         orientation_results = detector.analyze_document_orientation(orig_images, repro_images)
+         
+         overall_orientation = orientation_results[&#39;overall_analysis&#39;]
+         print(f&quot;✓ Most common rotation: {overall_orientation[&#39;most_common_coarse_rotation&#39;]}°&quot;)
+         print(f&quot;✓ Average confidence: {overall_orientation[&#39;average_confidence&#39;]:.3f}&quot;)
+         print(f&quot;✓ Average fine adjustment: {overall_orientation[&#39;average_fine_adjustment&#39;]:.1f}°&quot;)
+         
+         # Step 3: Align images
+         print(&quot;\n3. ALIGNING IMAGES&quot;)
+         print(&quot;-&quot; * 40)
+         
+         alignment_dir = case_dir / &quot;03_aligned_images&quot;
+         aligner = ImageAligner(output_dir=str(alignment_dir))
+         aligner.target_size_method = &quot;original&quot;  # Use original size as reference
+         alignment_results = aligner.align_document_pair(orig_images, repro_images, orientation_results)
+         
+         overall_alignment = alignment_results[&#39;overall_statistics&#39;]
+         print(f&quot;✓ Pages aligned: {alignment_results[&#39;total_pages&#39;]}&quot;)
+         print(f&quot;✓ Average SSIM after alignment: {overall_alignment[&#39;average_ssim&#39;]:.4f}&quot;)
+         print(f&quot;✓ Average correlation after alignment: {overall_alignment[&#39;average_correlation&#39;]:.4f}&quot;)
+         
+         # Step 4: Prepare results
+         print(&quot;\n4. PREPARING RESULTS&quot;)
+         print(&quot;-&quot; * 40)
+         
+         aligned_pairs = alignment_results[&#39;aligned_image_pairs&#39;]
+         print(f&quot;✓ {len(aligned_pairs)} aligned image pairs ready for AOI analysis&quot;)
+         
+         # Create summary visualization
+         summary_path = create_pipeline_summary(case_dir, case_name, conversion_info, 
+                                              orientation_results, alignment_results)
+         print(f&quot;✓ Pipeline summary saved: {summary_path}&quot;)
+         
+         return {
+             &#39;success&#39;: True,
+             &#39;case_name&#39;: case_name,
+             &#39;output_directory&#39;: str(case_dir),
+             &#39;conversion_info&#39;: conversion_info,
+             &#39;orientation_results&#39;: orientation_results,
+             &#39;alignment_results&#39;: alignment_results,
+             &#39;aligned_image_pairs&#39;: aligned_pairs,
+             &#39;summary_visualization&#39;: summary_path
+         }
+         
+     except Exception as e:
+         print(f&quot;✗ Error in pipeline: {e}&quot;)
+         return {
+             &#39;success&#39;: False,
+             &#39;case_name&#39;: case_name,
+             &#39;error&#39;: str(e)
+         }
+ 
+ 
+ def create_pipeline_summary(case_dir: Path, case_name: str, conversion_info: dict,
+                           orientation_results: dict, alignment_results: dict) -&gt; str:
+     &quot;&quot;&quot;Create a summary visualization of the complete pipeline.&quot;&quot;&quot;
+     
+     import matplotlib.pyplot as plt
+     import matplotlib.patches as patches
+     
+     fig, axes = plt.subplots(2, 2, figsize=(16, 12))
+     fig.suptitle(f&#39;Complete Alignment Pipeline Summary: {case_name}&#39;, 
+                  fontsize=16, fontweight=&#39;bold&#39;)
+     
+     # Conversion summary
+     axes[0, 0].axis(&#39;off&#39;)
+     axes[0, 0].set_title(&#39;1. PDF Conversion&#39;, fontweight=&#39;bold&#39;)
+     
+     conv_text = f&quot;&quot;&quot;
+ PDF Conversion Results:
+ 
+ Method: {conversion_info.get(&#39;method&#39;, &#39;N/A&#39;)}
+ DPI: {conversion_info.get(&#39;dpi&#39;, &#39;N/A&#39;)}
+ Original Pages: {conversion_info.get(&#39;original_pages&#39;, &#39;N/A&#39;)}
+ Reproduction Pages: {conversion_info.get(&#39;reproduction_pages&#39;, &#39;N/A&#39;)}
+ 
+ Original Dimensions: {conversion_info.get(&#39;original_dimensions&#39;, {}).get(&#39;width&#39;, &#39;N/A&#39;)}x{conversion_info.get(&#39;original_dimensions&#39;, {}).get(&#39;height&#39;, &#39;N/A&#39;)}
+ Reproduction Dimensions: {conversion_info.get(&#39;reproduction_dimensions&#39;, {}).get(&#39;width&#39;, &#39;N/A&#39;)}x{conversion_info.get(&#39;reproduction_dimensions&#39;, {}).get(&#39;height&#39;, &#39;N/A&#39;)}
+ 
+ Dimension Mismatch: {&#39;Yes&#39; if conversion_info.get(&#39;dimension_mismatch&#39;, False) else &#39;No&#39;}
+     &quot;&quot;&quot;
+     
+     axes[0, 0].text(0.05, 0.95, conv_text, fontsize=10, verticalalignment=&#39;top&#39;,
+                    fontfamily=&#39;monospace&#39;, transform=axes[0, 0].transAxes)
+     
+     # Orientation detection summary
+     axes[0, 1].axis(&#39;off&#39;)
+     axes[0, 1].set_title(&#39;2. Orientation Detection&#39;, fontweight=&#39;bold&#39;)
+     
+     overall_orient = orientation_results[&#39;overall_analysis&#39;]
+     orient_text = f&quot;&quot;&quot;
+ Orientation Detection Results:
+ 
+ Most Common Rotation: {overall_orient[&#39;most_common_coarse_rotation&#39;]}°
+ Rotation Consistency: {overall_orient[&#39;rotation_consistency&#39;]:.1%}
+ Average Confidence: {overall_orient[&#39;average_confidence&#39;]:.3f}
+ Average Fine Adjustment: {overall_orient[&#39;average_fine_adjustment&#39;]:.1f}°
+ 
+ Quality Distribution:
+   HIGH: {overall_orient[&#39;quality_distribution&#39;][&#39;HIGH&#39;]} pages
+   MEDIUM: {overall_orient[&#39;quality_distribution&#39;][&#39;MEDIUM&#39;]} pages
+   LOW: {overall_orient[&#39;quality_distribution&#39;][&#39;LOW&#39;]} pages
+ 
+ Pages with Warnings: {overall_orient[&#39;pages_with_warnings&#39;]}
+     &quot;&quot;&quot;
+     
+     axes[0, 1].text(0.05, 0.95, orient_text, fontsize=10, verticalalignment=&#39;top&#39;,
+                    fontfamily=&#39;monospace&#39;, transform=axes[0, 1].transAxes)
+     
+     # Alignment summary
+     axes[1, 0].axis(&#39;off&#39;)
+     axes[1, 0].set_title(&#39;3. Image Alignment&#39;, fontweight=&#39;bold&#39;)
+     
+     overall_align = alignment_results[&#39;overall_statistics&#39;]
+     align_text = f&quot;&quot;&quot;
+ Image Alignment Results:
+ 
+ Pages Processed: {overall_align[&#39;pages_processed&#39;]}
+ Size Normalization: {alignment_results[&#39;alignment_settings&#39;][&#39;normalize_sizes&#39;]}
+ Target Size Method: {alignment_results[&#39;alignment_settings&#39;][&#39;target_size_method&#39;]}
+ 
+ Quality Metrics (After Alignment):
+   Average SSIM: {overall_align[&#39;average_ssim&#39;]:.4f}
+   SSIM Std Dev: {overall_align[&#39;std_ssim&#39;]:.4f}
+   Average Correlation: {overall_align[&#39;average_correlation&#39;]:.4f}
+   Correlation Std Dev: {overall_align[&#39;std_correlation&#39;]:.4f}
+   Average MSE: {overall_align[&#39;average_mse&#39;]:.2f}
+   Average PSNR: {overall_align[&#39;average_psnr&#39;]:.2f} dB
+     &quot;&quot;&quot;
+     
+     axes[1, 0].text(0.05, 0.95, align_text, fontsize=10, verticalalignment=&#39;top&#39;,
+                    fontfamily=&#39;monospace&#39;, transform=axes[1, 0].transAxes)
+     
+     # Overall assessment
+     axes[1, 1].axis(&#39;off&#39;)
+     axes[1, 1].set_title(&#39;4. Overall Assessment&#39;, fontweight=&#39;bold&#39;)
+     
+     # Determine overall quality
+     ssim_quality = &quot;EXCELLENT&quot; if overall_align[&#39;average_ssim&#39;] &gt; 0.9 else \
+                   &quot;GOOD&quot; if overall_align[&#39;average_ssim&#39;] &gt; 0.8 else \
+                   &quot;FAIR&quot; if overall_align[&#39;average_ssim&#39;] &gt; 0.6 else &quot;POOR&quot;
+     
+     confidence_quality = &quot;HIGH&quot; if overall_orient[&#39;average_confidence&#39;] &gt; 0.8 else \
+                         &quot;MEDIUM&quot; if overall_orient[&#39;average_confidence&#39;] &gt; 0.6 else &quot;LOW&quot;
+     
+     assessment_text = f&quot;&quot;&quot;
+ Pipeline Assessment:
+ 
+ Alignment Quality: {ssim_quality}
+   (SSIM: {overall_align[&#39;average_ssim&#39;]:.4f})
+ 
+ Detection Confidence: {confidence_quality}
+   (Score: {overall_orient[&#39;average_confidence&#39;]:.3f})
+ 
+ Consistency: {&#39;GOOD&#39; if overall_orient[&#39;rotation_consistency&#39;] &gt; 0.8 else &#39;POOR&#39;}
+   (Rotation: {overall_orient[&#39;rotation_consistency&#39;]:.1%})
+ 
+ Ready for AOI Analysis: {&#39;YES&#39; if overall_align[&#39;average_ssim&#39;] &gt; 0.6 else &#39;NO&#39;}
+ 
+ Recommendations:
+     &quot;&quot;&quot;
+     
+     # Add recommendations
+     recommendations = overall_orient.get(&#39;recommendations&#39;, [])
+     if not recommendations:
+         assessment_text += &quot;\n  • No issues detected&quot;
+     else:
+         for rec in recommendations[:3]:  # Show first 3 recommendations
+             assessment_text += f&quot;\n  • {rec}&quot;
+     
+     axes[1, 1].text(0.05, 0.95, assessment_text, fontsize=10, verticalalignment=&#39;top&#39;,
+                    fontfamily=&#39;monospace&#39;, transform=axes[1, 1].transAxes)
+     
+     # Add colored border based on overall quality
+     if overall_align[&#39;average_ssim&#39;] &gt; 0.8:
+         border_color = &#39;green&#39;
+     elif overall_align[&#39;average_ssim&#39;] &gt; 0.6:
+         border_color = &#39;orange&#39;
+     else:
+         border_color = &#39;red&#39;
+     
+     rect = patches.Rectangle((0, 0), 1, 1, linewidth=4, edgecolor=border_color,
+                            facecolor=&#39;none&#39;, transform=axes[1, 1].transAxes)
+     axes[1, 1].add_patch(rect)
+     
+     plt.tight_layout()
+     
+     # Save summary
+     summary_path = case_dir / f&quot;pipeline_summary_{case_name.lower()}.png&quot;
+     plt.savefig(summary_path, dpi=150, bbox_inches=&#39;tight&#39;)
+     plt.close()
+     
+     return str(summary_path)
+ 
+ 
+ def main():
+     &quot;&quot;&quot;Run complete alignment pipeline tests on all available test cases.&quot;&quot;&quot;
+     
+     # Test file paths (relative to scanner root)
+     test_dir = Path(&quot;../../test&quot;)
+     original_pdf = test_dir / &quot;orig&quot; / &quot;1427004500479387.pdf&quot;
+     
+     test_cases = {
+         &quot;OK&quot;: test_dir / &quot;scan&quot; / &quot;1427004500479387_SCAN_OK.pdf&quot;,
+         &quot;NG&quot;: test_dir / &quot;scan&quot; / &quot;1427004500479387_SCAN_NG.pdf&quot;, 
+         &quot;BK&quot;: test_dir / &quot;scan&quot; / &quot;1427004500479387_SCAN_BK.pdf&quot;
+     }
+     
+     # Verify files exist
+     if not original_pdf.exists():
+         print(f&quot;Error: Original file not found: {original_pdf}&quot;)
+         sys.exit(1)
+     
+     available_cases = {}
+     for case_name, case_path in test_cases.items():
+         if case_path.exists():
+             available_cases[case_name] = case_path
+         else:
+             print(f&quot;Warning: Test case {case_name} not found: {case_path}&quot;)
+     
+     if not available_cases:
+         print(&quot;Error: No test cases found!&quot;)
+         sys.exit(1)
+     
+     # Run complete pipeline for each test case
+     results = {}
+     
+     for case_name, case_path in available_cases.items():
+         result = test_complete_alignment_pipeline(
+             str(original_pdf), str(case_path), case_name
+         )
+         results[case_name] = result
+     
+     # Print comparative summary
+     print_final_summary(results)
+ 
+ 
+ def print_final_summary(results: dict):
+     &quot;&quot;&quot;Print final summary of all test cases.&quot;&quot;&quot;
+     
+     print(f&quot;\n{&#39;=&#39;*100}&quot;)
+     print(&quot;COMPLETE ALIGNMENT PIPELINE SUMMARY&quot;)
+     print(f&quot;{&#39;=&#39;*100}&quot;)
+     
+     # Table header
+     print(f&quot;{&#39;Case&#39;:&lt;8} {&#39;Success&#39;:&lt;8} {&#39;Pages&#39;:&lt;6} {&#39;Rotation&#39;:&lt;9} {&#39;SSIM&#39;:&lt;8} {&#39;Confidence&#39;:&lt;11} {&#39;Quality&#39;:&lt;12}&quot;)
+     print(f&quot;{&#39;-&#39;*8} {&#39;-&#39;*8} {&#39;-&#39;*6} {&#39;-&#39;*9} {&#39;-&#39;*8} {&#39;-&#39;*11} {&#39;-&#39;*12}&quot;)
+     
+     successful_cases = []
+     
+     for case_name, result in results.items():
+         if result[&#39;success&#39;]:
+             alignment = result[&#39;alignment_results&#39;][&#39;overall_statistics&#39;]
+             orientation = result[&#39;orientation_results&#39;][&#39;overall_analysis&#39;]
+             
+             pages = alignment[&#39;pages_processed&#39;]
+             rotation = f&quot;{orientation[&#39;most_common_coarse_rotation&#39;]}°&quot;
+             ssim = f&quot;{alignment[&#39;average_ssim&#39;]:.3f}&quot;
+             confidence = f&quot;{orientation[&#39;average_confidence&#39;]:.3f}&quot;
+             
+             # Overall quality assessment
+             if alignment[&#39;average_ssim&#39;] &gt; 0.8:
+                 quality = &quot;EXCELLENT&quot;
+             elif alignment[&#39;average_ssim&#39;] &gt; 0.6:
+                 quality = &quot;GOOD&quot;
+             else:
+                 quality = &quot;POOR&quot;
+             
+             print(f&quot;{case_name:&lt;8} {&#39;✓&#39;:&lt;8} {pages:&lt;6} {rotation:&lt;9} {ssim:&lt;8} {confidence:&lt;11} {quality:&lt;12}&quot;)
+             successful_cases.append((case_name, result))
+         else:
+             print(f&quot;{case_name:&lt;8} {&#39;✗&#39;:&lt;8} {&#39;N/A&#39;:&lt;6} {&#39;N/A&#39;:&lt;9} {&#39;N/A&#39;:&lt;8} {&#39;N/A&#39;:&lt;11} {&#39;FAILED&#39;:&lt;12}&quot;)
+     
+     if successful_cases:
+         print(f&quot;\n{&#39;=&#39;*100}&quot;)
+         print(&quot;ALIGNED IMAGE PAIRS READY FOR AOI ANALYSIS&quot;)
+         print(f&quot;{&#39;=&#39;*100}&quot;)
+         
+         for case_name, result in successful_cases:
+             aligned_pairs = result[&#39;aligned_image_pairs&#39;]
+             print(f&quot;\n{case_name} Case:&quot;)
+             print(f&quot;  • {len(aligned_pairs)} aligned image pairs available&quot;)
+             print(f&quot;  • Output directory: {result[&#39;output_directory&#39;]}&quot;)
+             print(f&quot;  • Aligned images saved in: {result[&#39;output_directory&#39;]}/03_aligned_images/&quot;)
+             
+             # Show per-page alignment quality
+             for pair in aligned_pairs[:3]:  # Show first 3 pages
+                 page_num = pair[&#39;page&#39;]
+                 ssim = pair[&#39;alignment_quality&#39;][&#39;ssim&#39;]
+                 rotation = pair[&#39;rotation_applied&#39;]
+                 print(f&quot;    Page {page_num}: SSIM={ssim:.3f}, Rotation={rotation:.1f}°&quot;)
+             
+             if len(aligned_pairs) &gt; 3:
+                 print(f&quot;    ... and {len(aligned_pairs) - 3} more pages&quot;)
+         
+         print(f&quot;\n{&#39;=&#39;*100}&quot;)
+         print(&quot;NEXT STEPS&quot;)
+         print(f&quot;{&#39;=&#39;*100}&quot;)
+         print(&quot;The aligned image pairs are now ready for:&quot;)
+         print(&quot;• Brightness/contrast analysis&quot;)
+         print(&quot;• Scratch and artifact detection&quot;) 
+         print(&quot;• OCR readability assessment&quot;)
+         print(&quot;• Any other AOI analysis modules&quot;)
+         print(&quot;\nAccess aligned images via: result[&#39;aligned_image_pairs&#39;]&quot;)
+         print(&quot;Each pair contains: &#39;original&#39; and &#39;reproduction&#39; numpy arrays&quot;)
+ 
+ 
+ if __name__ == &quot;__main__&quot;:
+     main()
```

</details>

</tool-use>

---

<tool-use data-tool-type="generic" data-tool-name="todo_write">
<details><summary>Tool use: **todo_write** • Todo List</summary>

- [x] Analyze current system architecture and identify improvement areas
- [ ] Design comprehensive AOI framework with configurable inspection modules
- [ ] Plan GUI interface with preview windows and real-time visualization
- [ ] Design advanced defect detection algorithms (positioning, content, quality)
- [ ] Plan flexible configuration system for thresholds and inspection parameters
- [x] Design image processing pipeline for microfilm defect detection
- [ ] Implement overexposure/underexposure detection algorithms
- [ ] Implement scratch and artifact detection
- [ ] Implement OCR-based readability assessment
- [x] Create brightness/contrast analysis script for test PDFs
- [x] Create orientation detection module for PDF alignment
- [x] Create image alignment module that applies orientation corrections

</details>
</tool-use>

---

Perfect! Now you have a **complete alignment system** that actually aligns the images. Here's what the new system provides:

## **Complete Alignment Pipeline:**

### **1. Image Aligner (`image_aligner.py`)**
- **Applies rotation corrections** detected by the orientation detector
- **Normalizes image sizes** to ensure consistent dimensions
- **Calculates alignment quality metrics** (SSIM, correlation, MSE, PSNR)
- **Saves aligned images** to disk for inspection
- **Creates before/after visualizations** showing alignment improvement

### **2. Complete Pipeline Test (`test_complete_alignment.py`)**
- **Full workflow**: PDF conversion → Orientation detection → Image alignment
- **Provides aligned image pairs** ready for AOI analysis
- **Comprehensive reporting** with quality metrics
- **Visual summaries** of the entire pipeline

## **Key Features of the Alignment System:**

### **Rotation Correction:**
- **90° increments**: Efficient using OpenCV's built-in rotation
- **Fine angles**: Affine transformation with canvas expansion (no cropping)
- **Quality preservation**: Maintains image content during rotation

### **Size Normalization Options:**
- **"original"**: Use original document size as reference
- **"reproduction"**: Use reproduction size as reference  
- **"largest"**: Use largest dimensions from either
- **"custom"**: Specify exact target size

### **Quality Assessment:**
- **SSIM**: Structural similarity (higher = better alignment)
- **Correlation**: Cross-correlation coefficient
- **MSE**: Mean squared error (lower = better)
- **PSNR**: Peak signal-to-noise ratio

### **Output:**
- **Aligned image pairs**: Ready for subsequent AOI analysis
- **Before/after comparisons**: Visual verification of alignment
- **Quality metrics**: Quantitative assessment of alignment success
- **Saved images**: Individual aligned images for manual inspection

## **How to Use:**

```bash
cd scanner/AOI/orientation
python test_complete_alignment.py
```

This will:
1. **Convert** all your PDFs (OK, NG, BK) to images
2. **Detect** orientation for each page
3. **Apply** rotation corrections to align reproduction with original
4. **Normalize** image sizes for consistent comparison
5. **Provide** aligned image pairs ready for AOI analysis
6. **Generate** comprehensive reports and visualizations

## **What You Get:**

After running the complete pipeline, you'll have:

```python
# Access aligned image pairs like this:
aligned_pairs = result['aligned_image_pairs']

for pair in aligned_pairs:
    page_num = pair['page']
    original_img = pair['original']      # Aligned original image
    reproduction_img = pair['reproduction']  # Aligned reproduction image
    rotation_applied = pair['rotation_applied']  # Rotation that was applied
    quality_metrics = pair['alignment_quality']  # SSIM, correlation, etc.
    
    # Now you can do AOI analysis on properly aligned images!
    # brightness_analysis(original_img, reproduction_img)
    # scratch_detection(original_img, reproduction_img)
    # ocr_comparison(original_img, reproduction_img)
```

The aligned images are now **perfectly oriented** and **size-matched**, making all subsequent AOI analysis much more accurate and reliable!

---

